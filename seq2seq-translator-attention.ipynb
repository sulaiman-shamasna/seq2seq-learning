{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a38f66-a16c-4aa6-b33a-43a1c38995fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6380ead-cc47-4e35-bb07-6d22a5be54cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# Not setting this led to the following error\n",
    "# _Derived_]RecvAsync is cancelled.   \n",
    "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a201b813-5b9c-4653-bc00-1f74d26f11bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Make sure the zip file has been downloaded\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    raise FileNotFoundError(\n",
    "        \"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25db97fc-fe57-462b-a975-c2f6bdfcf911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (277891, 2)\n",
      "(clean) df.shape = (277205, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))\n",
    "\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "df = df.iloc[clean_inds]\n",
    "\n",
    "print('(clean) df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77269af0-7ab7-4385-a16e-38954ca94cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277882</th>\n",
       "      <td>Remember that the purpose of the Tatoeba Proje...</td>\n",
       "      <td>Es gilt zu bedenken, dass es das Anliegen des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277883</th>\n",
       "      <td>When I was younger, I hated going to weddings....</td>\n",
       "      <td>Als ich jünger war, hasste ich es, auf Hochzei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277884</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der deine Herkunft nicht kennt, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277885</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277887</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn einem von jemandem, der nicht weiß, woher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       EN  \\\n",
       "277882  Remember that the purpose of the Tatoeba Proje...   \n",
       "277883  When I was younger, I hated going to weddings....   \n",
       "277884  If someone who doesn't know your background sa...   \n",
       "277885  If someone who doesn't know your background sa...   \n",
       "277887  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                       DE  \n",
       "277882  Es gilt zu bedenken, dass es das Anliegen des ...  \n",
       "277883  Als ich jünger war, hasste ich es, auf Hochzei...  \n",
       "277884  Wenn jemand, der deine Herkunft nicht kennt, s...  \n",
       "277885  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
       "277887  Wenn einem von jemandem, der nicht weiß, woher...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528cc3ca-0fb4-4047-a0b9-122e10418768",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "df = df.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04dcb70-a0d6-4a7f-9fc3-8cf608db2b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (5000, 2)\n",
      "valid_df.shape = (5000, 2)\n",
      "train_df.shape = (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "857b57cc-5fb9-4a91-99da-0cb2a1a01826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9228\n",
      "to     8700\n",
      "I      8620\n",
      "the    6766\n",
      "you    6136\n",
      "a      5741\n",
      "is     4141\n",
      "in     2639\n",
      "of     2470\n",
      "was    2380\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2218\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom       9713\n",
      "Ich       7964\n",
      "ist       4735\n",
      "nicht     4616\n",
      "zu        3606\n",
      "Sie       3441\n",
      "du        3132\n",
      "das       2987\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2483\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a flattened list from English words\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "# Create a flattened list of German words\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "\n",
    "# Get the vocabulary size of words appearing more than or equal to 10 times\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \n",
    "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
    "    \n",
    "    # Generate a counter object i.e. dict word -> frequency\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    # Create a pandas series from the counter, then sort most frequent to least\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2ba71b-8a1f-4c1f-87ce-aae9ab1a3c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.294025\n",
      "std          2.542850\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39584.000000\n",
      "mean         6.184671\n",
      "std          2.284073\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.332250\n",
      "std          2.536094\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         52.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39227.000000\n",
      "mean         8.253116\n",
      "std          2.231582\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \n",
    "    \"\"\" Print the summary stats of the sequence length \"\"\"\n",
    "    \n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    # Get the quantiles at given marks\n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    # Print the summary stats of the data between the defined quantlies\n",
    "    print(seq_length_ser[(seq_length_ser >= p_01) & (seq_length_ser < p_99)].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb736a8d-5d7a-42fd-b626-f411f959cab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2218\n",
      "DE vocabulary size: 2483\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b053c549-a758-4faf-b947-1556d9daa0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class DecoderRNNAttentionWrapper(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, cell, units, **kwargs):\n",
    "        self._cell = cell\n",
    "        self.units = units\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        # Weight matrices of the attention layer\n",
    "        # W_a is a [s, self.units] sized matrix\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], self.units)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        # U_a is a [h, self.units] sized matrix\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((self._cell.units, self.units)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((self.units, 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "    \n",
    "    # Code listing 12.1\n",
    "    def call(self, inputs, initial_state, training=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        \n",
    "        encoder_outputs, decoder_inputs = inputs\n",
    "\n",
    "        def _step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * de_in_dim)\n",
    "            states: [(batchsize * de_latent_dim)]\n",
    "            \"\"\"\n",
    "            \n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "            \n",
    "            encoder_full_seq = states[-1]\n",
    "            \n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_h = K.dot(encoder_full_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"                        \n",
    "            U_a_dot_s = K.expand_dims(K.dot(states[0], self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "                      \n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Wh_plus_Us = K.tanh(W_a_dot_h + U_a_dot_s)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Wh_plus_Us, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            \"\"\" Computing c_i using e_i and h_j (for all j) \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_full_seq * K.expand_dims(e_i, -1), axis=1)\n",
    "            \n",
    "            \"\"\" Concat the current input and c_i and feed it to the RNN \"\"\"\n",
    "            s, states = self._cell(K.concatenate([inputs, c_i], axis=-1), states)              \n",
    "        \n",
    "            return (s, e_i), states\n",
    "\n",
    "        \"\"\" Computing outputs \"\"\"\n",
    "        \n",
    "        last_out, attn_outputs, _ = K.rnn(\n",
    "            _step, decoder_inputs, [initial_state], constants=[encoder_outputs]\n",
    "        )\n",
    "\n",
    "        # attn_out => (batch_size, de_seq_len, de_hidden_size)\n",
    "        # attn_energy => (batch_size, de_seq_len, en_seq_len)\n",
    "        attn_out, attn_energy = attn_outputs        \n",
    "\n",
    "        return attn_out, attn_energy\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units, \"_cell\": self._cell}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc503d1-5485-41f1-8524-821b0e42860f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac0c32-ac10-446b-b28a-fd2c63814358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd02f6-ae2f-437f-9ada-a752a85289c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8b395-2f90-4c4f-98ce-b71ca2e12036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292f2e5-e515-414c-a192-159533601943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efdea4-ad20-459e-ba60-19168b38c22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c27b8-0eb8-4356-bc08-47af714bb046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23550195-0b5c-4ddb-817f-401051eb0dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0cafd-6852-4317-a7a9-dd63b5ff1e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa660c-431a-468d-ba5a-7491b9ac9f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc453d8-22f5-416a-a35f-e4f48505bd90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
