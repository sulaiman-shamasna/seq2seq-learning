{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f2367d-5df8-4a3c-b080-c50a0745bc83",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Learning\n",
    "---\n",
    "This notebook illustrates the detailed steps of english-german translation model as a *sequence to sequence* problem.\n",
    "\n",
    "***Reference***: [TensorFlow in Action](https://www.google.de/books/edition/TensorFlow_in_Action/JYyKEAAAQBAJ?hl=en&gbpv=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57b1a6a-4649-44a5-9a1a-b30b801f4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b7e53-1f40-4001-8544-abf638457f31",
   "metadata": {},
   "source": [
    "Download [deu-eng dataset](http://www.manythings.org/anki/deu-eng.zip) manually and locate it in the ```\\data``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac1e4a6-01bd-4486-b2f8-8d1dafb8e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Make sure the zip file has been downloaded\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    raise FileNotFoundError(\n",
    "    \"Uh oh! Did you download the deu-eng.zip from http:/ /www.manythings.org/anki/deu-eng.zip manually and place it in the /data folder?\")\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c6becf-b3ea-4b80-a44c-508c5d7c613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf36fd4f-39b2-438b-9793-2e45448a1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (277891, 2)\n"
     ]
    }
   ],
   "source": [
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86cdb4c-065a-4d7f-9da7-3de4aa12a299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "df = df.iloc[clean_inds]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62f909e-a092-4d48-8682-873374175c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "random_seed= 4321\n",
    "df = df.sample(n=n_samples, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2cc713-cd49-4c18-bdc9-a011cfd50cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef156f1c-a81d-42a8-9b6c-dc3f539611d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 10% examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Randomly sample 10% examples from the remaining randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295e618-ce0d-48c6-81bd-74ae22053ed1",
   "metadata": {},
   "source": [
    "### Analyzing the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d1aa725-8dce-4b8c-be55-23357c9779c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9228\n",
      "to     8700\n",
      "I      8620\n",
      "the    6766\n",
      "you    6136\n",
      "a      5741\n",
      "is     4141\n",
      "in     2639\n",
      "of     2470\n",
      "was    2380\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2218\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom       9713\n",
      "Ich       7964\n",
      "ist       4735\n",
      "nicht     4616\n",
      "zu        3606\n",
      "Sie       3441\n",
      "du        3132\n",
      "das       2987\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2483\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the vocabulary size above a certain threshold\n",
    "    \"\"\"\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    freq_df = pd.Series(\n",
    "    list(counter.values()),\n",
    "    index=list(counter.keys())\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(freq_df.head(n=10))\n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "    return n_vocab\n",
    "\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e751ac-0194-4df8-b383-b5596daca510",
   "metadata": {},
   "source": [
    "### Analyzing the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8da83ca8-2654-4fdf-9c68-f5d13d506fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \"\"\"\n",
    "    Print the summary stats of the sequence length\n",
    "    \"\"\"\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    \n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    print(seq_length_ser[\n",
    "    (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n",
    "    ].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b5a8c9e-c57c-4faa-84d8-39a78f9cc40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.294025\n",
      "std          2.542850\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39584.000000\n",
      "mean         6.184671\n",
      "std          2.284073\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.332250\n",
      "std          2.536094\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         52.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39227.000000\n",
      "mean         8.253116\n",
      "std          2.231582\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13687aae-3acc-48d0-9e9a-1f9f318c1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2218\n",
      "DE vocabulary size: 2483\n",
      "\n",
      "\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "print(\"\\n\")\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16721691-95c8-4521-8bf8-be1bb4001cb3",
   "metadata": {},
   "source": [
    "### Writing an English-German seq2seq Machine Translator\n",
    "---\n",
    "\n",
    "Machine translation involves transforming text from one language to another. In this notebook, I focus on creating an *English-to-German* translator using a *sequence-to-sequence* (*seq2seq*) deep learning model.\n",
    "\n",
    "**Seq2Seq Model Architecture**. The seq2seq model consists of two primary components:\n",
    "- *Encoder:*\n",
    "    - Processes the input (English) text and generates a fixed-length context vector, also known as a \"*[thought vector](https://wiki.pathmind.com/thought-vectors)*.\"\n",
    "    - Encodes the input sequence into a hidden representation that summarizes its semantic and syntactic content.\n",
    "- *Decoder:*\n",
    "  - Takes the context vector produced by the encoder as input.\n",
    "  - Decodes it to produce the output sequence (German text).\n",
    "\n",
    "Both the encoder and decoder are *recurrent neural networks* (*RNNs*), making them suitable for handling sequential data of arbitrary lengths.\n",
    "\n",
    "**Key Challenges in Seq2Seq Learning**:\n",
    "- *Variable Lengths*: The input and output sequences often differ in length. For example, the number of words in a translation might be fewer or greater than the source text.\n",
    "- *Mapping Arbitrary Sequences*: The model must map an input sequence of arbitrary length to an output sequence of arbitrary length while preserving contextual meaning.\n",
    "\n",
    "![encoder-decoder-machine-translation](plots/enc-dec.svg)\n",
    "\n",
    "The *encoder* in our sequence-to-sequence model is built using a *Gated Recurrent Unit* (*GRU*), a type of *RNN*. Its role is to process the input sequence and produce a fixed-size output that summarizes the sequence's information. The *GRU* processes each element of the input sequence step by step, updating its hidden state based on the current input and the previous hidden state. After processing the entire sequence, the final hidden state of the *GRU* serves as the context vector, which encapsulates the semantic and syntactic information of the input.\n",
    "\n",
    "The *decoder*, which is also based on a *GRU*, further processes the context vector to generate the target sequence. In addition to the *GRU*, the *decoder* incorporates *Dense layers*, which play a critical role in producing the final output. The Dense layers map the *GRU's* outputs to the target vocabulary, generating a probability distribution over possible words at each time step. A key feature of the *decoder* is that the weights of the Dense layers are shared across time steps. This means that, similar to how the *GRU* updates and reuses the same weights for each input in the sequence, the Dense layers also reuse their weights for predicting each word in the output sequence. This approach ensures consistency and efficiency in processing sequential data.\n",
    "\n",
    "![encoder-decoder-machine-translation](plots/gru.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d283e1-b6f5-4924-972e-947583e9bf04",
   "metadata": {},
   "source": [
    "### The TextVectorization layer\n",
    "---\n",
    "\n",
    "The TextVectorization layer takes in a string, tokenizes it, and converts the tokens to *IDs* by means of a vocabulary (or dictionary) lookup. It takes a list of strings (or an array of strings) as the input, where each string can be a *word/phrase/sentence* (and so on). Then it learns the vocabulary from that corpus. Finally, the layer can be used to convert a list of strings to a tensor that contains a sequence of token IDs for each string in the list provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac444c4a-49c3-4050-8f62-e470046b8b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "en_vectorize_layer = TextVectorization(\n",
    "max_tokens=en_vocab,\n",
    "output_mode='int',\n",
    "output_sequence_length=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a38749-37ac-4d05-905c-304bbb18c719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('tom'), np.str_('to'), np.str_('i'), np.str_('you'), np.str_('the'), np.str_('a'), np.str_('is'), np.str_('that')]\n"
     ]
    }
   ],
   "source": [
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2712c06-773a-45b2-896b-d1e69f204605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2218\n"
     ]
    }
   ],
   "source": [
    "print(len(en_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e390fd-79af-4a3d-ad2a-3d41abd2e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = tf.keras.models.Sequential()\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "input_data = [[\"run\"], [\"how are you\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5dfd20f-51d4-426a-a52d-72dc31027c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B8389EEB00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Input data: \n",
      "[b'run' b'how are you' b'ectoplasmic residue']\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[3 0 0 0 0 0 0 0 0 0]\n",
      " [5 7 2 0 0 0 0 0 0 0]\n",
      " [6 4 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Vocabulary size and sequence length\n",
    "en_vocab = 1000\n",
    "\n",
    "# Define the TextVectorization layer\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=10  # Adjust sequence length as needed\n",
    ")\n",
    "\n",
    "# Adapt the TextVectorization layer to a sample dataset\n",
    "sample_text = [\"run\", \"how are you\", \"ectoplasmic residue\"]\n",
    "en_vectorize_layer.adapt(sample_text)\n",
    "\n",
    "# Create a toy model with the TextVectorization layer\n",
    "toy_model = Sequential([en_vectorize_layer])\n",
    "\n",
    "# Input data (ensure it's a TensorFlow tensor of strings)\n",
    "input_data = tf.constant([\"run\", \"how are you\", \"ectoplasmic residue\"])\n",
    "\n",
    "# Predict token IDs using the model\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "# Display results\n",
    "print(\"Input data: \\n{}\\n\".format(input_data.numpy()))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a2633-5bd5-40e9-bd03-f8e9b02e66cd",
   "metadata": {},
   "source": [
    "### Defining the text vectorizers for the encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e613088-a7fe-4523-963e-7eaa53c8ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(\n",
    "    corpus, n_vocab, max_length=None, return_vocabulary=True, name=None\n",
    "    ):\n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "                                                        max_tokens=n_vocab+2,\n",
    "                                                        output_mode='int',\n",
    "                                                        output_sequence_length=max_length,\n",
    "                                                        )\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    if not return_vocabulary:\n",
    "        return tf.keras.models.Model(\n",
    "    inputs=inp, outputs=vectorized_out, name=name\n",
    "    )\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b06fd35-c747-4456-ba8d-51667a7be9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\n",
    "max_length=en_seq_length, name='en_vectorizer'\n",
    ")\n",
    "\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n",
    "max_length=de_seq_length-1, name='de_vectorizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045b454-4dfb-4e4e-bca0-a933635e300e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969164a-2db8-4bc4-afe5-77c886637310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4eb78-91c6-46a3-92a9-fdb9298d8c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b79bff-6bd6-4512-91ae-836d1085f504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
