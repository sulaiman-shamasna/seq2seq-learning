{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b803581-d8fb-47f1-9647-15a66f2fad93",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Learning: English-German Translator\n",
    "\n",
    "This notebook illustrates how to develop an English-to-German translation model as a sequence-to-sequence (seq2seq) learning problem using TensorFlow and Keras. This project demonstrates the core components of an encoder-decoder architecture with *BiLSTMs*, handling of text preprocessing, and evaluation using BLEU scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Sequence-to-Sequence Learning Overview\n",
    "\n",
    "Sequence-to-sequence (seq2seq) learning is a type of deep learning model that maps input sequences to output sequences of potentially different lengths. It has wide applications in:\n",
    "\n",
    "- Machine Translation (e.g., English-to-German)\n",
    "- Text Summarization\n",
    "- Chatbots and Conversational Agents\n",
    "\n",
    "Key components:\n",
    "- **Encoder**: Encodes the input sequence into a fixed-length context vector.\n",
    "- **Decoder**: Decodes the context vector to generate the target sequence.\n",
    "\n",
    "\n",
    "***Reference***: [TensorFlow in Action](https://www.google.de/books/edition/TensorFlow_in_Action/JYyKEAAAQBAJ?hl=en&gbpv=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab96504-1e7d-4489-8c35-a040e1054016",
   "metadata": {},
   "source": [
    "### **Step 1**: Download and Extract the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff7a8b5-c2a0-4e27-815b-1ea25619becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Ensure the required dataset is available and extracted\n",
    "def prepare_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the English-German dataset for translation.\n",
    "    Downloads and extracts the data if not already present.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing English and German sentences.\n",
    "    \"\"\"\n",
    "    data_dir = 'data'\n",
    "    zip_path = os.path.join(data_dir, 'deu-eng.zip')\n",
    "    extracted_path = os.path.join(data_dir, 'deu.txt')\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Please download 'deu-eng.zip' from \"\n",
    "            f\"http://www.manythings.org/anki/deu-eng.zip and place it in the {data_dir} folder.\"\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(extracted_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Dataset extracted.\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    df = pd.read_csv(extracted_path, delimiter='\\t', header=None, names=[\"EN\", \"DE\", \"Attribution\"])\n",
    "    df = df[[\"EN\", \"DE\"]]\n",
    "    return df\n",
    "\n",
    "df = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55574daa-31e4-4f56-a0bc-ddb2c91073e8",
   "metadata": {},
   "source": [
    "**Explanation**: This step ensures the required dataset is extracted for processing. If the dataset is not present in the expected location, it prompts the user to download it manually.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325d0df-c72f-4574-a0ab-1b246f0af276",
   "metadata": {},
   "source": [
    "### **Step 2**: Data Setup and Preprocessing\n",
    "---\n",
    "\n",
    "Download [deu-eng dataset](http://www.manythings.org/anki/deu-eng.zip) manually and locate it in the ```\\data``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb765ef7-fb5f-43b4-9ec5-e70383c9a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "\n",
    "print('df.shape = {}'.format(df.shape))\n",
    "\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "df = df.iloc[clean_inds]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01103c-7de1-411c-911e-2eb575762762",
   "metadata": {},
   "source": [
    "**Explanation**: In this step, we clean the data by removing rows with unwanted characters and retain only English and German sentence pairs for the translation task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0862044-6ebf-459b-86eb-072df1b974f5",
   "metadata": {},
   "source": [
    "### **Step 3**: Random Sampling and Token Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3a317-1fe6-4c0d-b263-c076d10ab47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "random_seed= 4321\n",
    "df = df.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token\n",
    "\n",
    "# Randomly sample 10% examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Randomly sample 10% examples from the remaining randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f41bf-32cd-4de5-b52a-96a666166eee",
   "metadata": {},
   "source": [
    "**Explanation**: This step prepares a subset of ```50,000``` sentences for the task, appends start (```sos```) and end (```eos```) tokens to each German sentence, and splits the data into training, validation, and test sets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df912329-842f-429c-90d0-ee5842b112da",
   "metadata": {},
   "source": [
    "### **Step 4**: Analyze Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b17e00-6fb3-4947-b926-400af1763877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \"\"\"\n",
    "    Print the summary stats of the sequence length\n",
    "    \"\"\"\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    \n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    print(seq_length_ser[\n",
    "    (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n",
    "    ].describe())\n",
    "\n",
    "\"\"\"\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "\n",
    "print_sequence_length(train_df[\"DE\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0efa8-3532-40f6-afef-135d3019df28",
   "metadata": {},
   "source": [
    "**Explanation**: Here, we analyze the sequence lengths for both corpora. The results help determine appropriate sequence lengths for the encoder and decoder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8a47d-6660-46b0-ab63-484f1c966d50",
   "metadata": {},
   "source": [
    "### **Step 5**: Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74d93d-ae69-4e43-ac28-d4e59f2d8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the vocabulary size above a certain threshold\n",
    "    \"\"\"\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    freq_df = pd.Series(\n",
    "    list(counter.values()),\n",
    "    index=list(counter.keys())\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(freq_df.head(n=10))\n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "    return n_vocab\n",
    "\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05abad-518a-4a73-beaa-66fd33cfeeb9",
   "metadata": {},
   "source": [
    "**Explanation**: We compute the vocabulary size for both languages, focusing on words that appear at least 10 times. This informs the TextVectorization step later in the pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46746d73-a993-4ddb-b133-02066e037147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13f8f0-fbbf-4c23-9673-f5839cbb2262",
   "metadata": {},
   "source": [
    "## *Writing an English-German seq2seq Machine Translator*\n",
    "---\n",
    "\n",
    "Machine translation involves transforming text from one language to another. In this notebook, I focus on creating an *English-to-German* translator using a *sequence-to-sequence* (*seq2seq*) deep learning model.\n",
    "\n",
    "**Seq2Seq Model Architecture**. The seq2seq model consists of two primary components:\n",
    "- *Encoder:*\n",
    "    - Processes the input (English) text and generates a fixed-length context vector, also known as a \"*[thought vector](https://wiki.pathmind.com/thought-vectors)*.\"\n",
    "    - Encodes the input sequence into a hidden representation that summarizes its semantic and syntactic content.\n",
    "- *Decoder:*\n",
    "  - Takes the context vector produced by the encoder as input.\n",
    "  - Decodes it to produce the output sequence (German text).\n",
    "\n",
    "Both the encoder and decoder are *recurrent neural networks* (*RNNs*), making them suitable for handling sequential data of arbitrary lengths.\n",
    "\n",
    "**Key Challenges in Seq2Seq Learning**:\n",
    "- *Variable Lengths*: The input and output sequences often differ in length. For example, the number of words in a translation might be fewer or greater than the source text.\n",
    "- *Mapping Arbitrary Sequences*: The model must map an input sequence of arbitrary length to an output sequence of arbitrary length while preserving contextual meaning.\n",
    "\n",
    "![encoder-decoder-machine-translation](plots/enc-dec.svg)\n",
    "\n",
    "The *encoder* in our sequence-to-sequence model is built using a *Gated Recurrent Unit* (*GRU*), a type of *RNN*. Its role is to process the input sequence and produce a fixed-size output that summarizes the sequence's information. The *GRU* processes each element of the input sequence step by step, updating its hidden state based on the current input and the previous hidden state. After processing the entire sequence, the final hidden state of the *GRU* serves as the context vector, which encapsulates the semantic and syntactic information of the input.\n",
    "\n",
    "The *decoder*, which is also based on a *GRU*, further processes the context vector to generate the target sequence. In addition to the *GRU*, the *decoder* incorporates *Dense layers*, which play a critical role in producing the final output. The Dense layers map the *GRU's* outputs to the target vocabulary, generating a probability distribution over possible words at each time step. A key feature of the *decoder* is that the weights of the Dense layers are shared across time steps. This means that, similar to how the *GRU* updates and reuses the same weights for each input in the sequence, the Dense layers also reuse their weights for predicting each word in the output sequence. This approach ensures consistency and efficiency in processing sequential data.\n",
    "\n",
    "![encoder-decoder-machine-translation](plots/gru.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcedb2-3687-4251-b8a9-43498ce0caaf",
   "metadata": {},
   "source": [
    "### **Step 6**: Text Vectorization\n",
    "\n",
    "The TextVectorization layer takes in a string, tokenizes it, and converts the tokens to *IDs* by means of a vocabulary (or dictionary) lookup. It takes a list of strings (or an array of strings) as the input, where each string can be a *word/phrase/sentence* (and so on). Then it learns the vocabulary from that corpus. Finally, the layer can be used to convert a list of strings to a tensor that contains a sequence of token IDs for each string in the list provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd86f4-b30a-4f1a-9822-00ca49b9d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def get_vectorizer(\n",
    "    corpus, n_vocab=100, max_length=None, return_vocabulary=True, name=None\n",
    "    ):\n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "                                                        max_tokens=n_vocab+2,\n",
    "                                                        output_mode='int',\n",
    "                                                        output_sequence_length=max_length,\n",
    "                                                        )\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    if not return_vocabulary:\n",
    "        return tf.keras.models.Model(\n",
    "    inputs=inp, outputs=vectorized_out, name=name\n",
    "    )\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\n",
    "max_length=en_seq_length, name='en_vectorizer'\n",
    ")\n",
    "\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n",
    "max_length=de_seq_length-1, name='de_vectorizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb05bda-4726-43bc-abf7-23a53e01f67e",
   "metadata": {},
   "source": [
    "**Explanation**: Text vectorization converts sentences into sequences of integer token IDs, which are compatible with neural network input layers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34688d65-8a92-4626-9059-8ef8ca56230c",
   "metadata": {},
   "source": [
    "### **Step 7**: Build the Encoder and the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a3dac-5185-4dd8-9720-2fb91ca4a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_encoder(n_vocab, vectorizer):\n",
    "    \"\"\"\n",
    "    Build the encoder for the seq2seq model.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # Text vectorization\n",
    "    vectorized_output = vectorizer(encoder_input)\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=n_vocab + 2, \n",
    "        output_dim=128, \n",
    "        mask_zero=True, \n",
    "        name='encoder_embedding'\n",
    "    )\n",
    "    embedded_output = embedding_layer(vectorized_output)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    bidirectional_gru = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, name='encoder_gru'), \n",
    "        name='encoder_bidirectional_gru'\n",
    "    )\n",
    "    encoder_output = bidirectional_gru(embedded_output)\n",
    "    \n",
    "    # Define encoder model\n",
    "    encoder_model = tf.keras.models.Model(\n",
    "        inputs=encoder_input, \n",
    "        outputs=encoder_output, \n",
    "        name='encoder'\n",
    "    )\n",
    "    return encoder_model\n",
    "\n",
    "\n",
    "def build_decoder(n_vocab, encoder, vectorizer):\n",
    "    \"\"\"\n",
    "    Build the decoder for the seq2seq model.\n",
    "    \"\"\"\n",
    "    # Encoder input for initializing the decoder's GRU state\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input_final')\n",
    "    decoder_initial_state = encoder(encoder_input)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='decoder_input')\n",
    "    \n",
    "    # Text vectorization for decoder input\n",
    "    vectorized_output = vectorizer(decoder_input)\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=n_vocab + 2, \n",
    "        output_dim=128, \n",
    "        mask_zero=True, \n",
    "        name='decoder_embedding'\n",
    "    )\n",
    "    embedded_output = embedding_layer(vectorized_output)\n",
    "    \n",
    "    # GRU layer\n",
    "    gru_layer = tf.keras.layers.GRU(\n",
    "        256, \n",
    "        return_sequences=True, \n",
    "        name='decoder_gru'\n",
    "    )\n",
    "    gru_output = gru_layer(embedded_output, initial_state=decoder_initial_state)\n",
    "    \n",
    "    # Dense layers for output\n",
    "    dense_layer_1 = tf.keras.layers.Dense(\n",
    "        512, \n",
    "        activation='relu', \n",
    "        name='decoder_dense_1'\n",
    "    )\n",
    "    dense_output_1 = dense_layer_1(gru_output)\n",
    "    \n",
    "    dense_layer_final = tf.keras.layers.Dense(\n",
    "        n_vocab + 2, \n",
    "        activation='softmax', \n",
    "        name='decoder_dense_final'\n",
    "    )\n",
    "    decoder_output = dense_layer_final(dense_output_1)\n",
    "    \n",
    "    # Define decoder model\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        inputs=[encoder_input, decoder_input], \n",
    "        outputs=decoder_output, \n",
    "        name='decoder'\n",
    "    )\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f710ebd-ceb8-43d0-bc83-782583211db6",
   "metadata": {},
   "source": [
    "**Explanation**: The *encoder* processes the input sequence using text vectorization, embeddings, and a bidirectional GRU layer. It produces a context vector representing the input sequence. The *decoder*, on the other hand takes the encoder's context vector and generates output sequences. It uses a GRU layer for decoding and dense layers for token probability distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e78fc-ff3b-4ec6-8d9c-cc883a7f8fcb",
   "metadata": {},
   "source": [
    "#### *Bidirectional RNN: Reading text forward and backward*\n",
    "---\n",
    "\n",
    "![rnn-bi-rnn](plots/rnn-bi-rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d71c1-f56d-4c29-93f7-ef2676d7fbd6",
   "metadata": {},
   "source": [
    "### **Step 8**: Build and Compile the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707b356-9a4b-411a-9bed-00ea6af3f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq2seq_model(encoder, decoder):\n",
    "    \"\"\"\n",
    "    Build the final seq2seq model using the pre-built encoder and decoder.\n",
    "    \"\"\"\n",
    "    # Encoder input\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input_final')\n",
    "    encoder_output = encoder(encoder_input)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='decoder_input')\n",
    "\n",
    "    # Use the decoder model\n",
    "    decoder_output = decoder([encoder_input, decoder_input])\n",
    "\n",
    "    # Define the seq2seq model\n",
    "    seq2seq_model = tf.keras.models.Model(\n",
    "        inputs=[encoder_input, decoder_input], \n",
    "        outputs=decoder_output, \n",
    "        name='seq2seq_model'\n",
    "    )\n",
    "    return seq2seq_model\n",
    "\n",
    "\n",
    "# Usage\n",
    "# Assuming `en_vocab`, `en_vectorizer`, and `de_vectorizer` are defined\n",
    "encoder = build_encoder(en_vocab, en_vectorizer)\n",
    "decoder = build_decoder(de_vocab, encoder, de_vectorizer)\n",
    "seq2seq_model = build_seq2seq_model(encoder, decoder)\n",
    "\n",
    "# Summaries\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060d46e-39a3-431a-8f28-e864689edba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure [UNK] is at the beginning of the vocabulary\n",
    "en_vocabulary = [v for v in en_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "en_vocabulary = ['[UNK]'] + en_vocabulary  # Add [UNK] at the start\n",
    "\n",
    "de_vocabulary = [v for v in de_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "de_vocabulary = ['[UNK]'] + de_vocabulary  # Add [UNK] at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edab6bc-638f-458c-9ccd-408ca7f09547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "seq2seq_model.compile(\n",
    "loss='sparse_categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0f92e-b83c-416c-930f-3591b3c97cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3253e1c-e023-4104-bd12-a02640026ed0",
   "metadata": {},
   "source": [
    "**Explanation**: The seq2seq model combines the encoder and decoder into a unified framework for training and inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca240dc5-69e5-457d-a0ce-e2797f9b353d",
   "metadata": {},
   "source": [
    "### **Step 9**: Prepare and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2cda29-617e-4b62-afb5-b28fb6c1840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    Create a data dictionary from the dataframes containing data\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for label, df in zip(\n",
    "        ['train', 'valid', 'test'], [train_df, valid_df, test_df]\n",
    "        ):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(\n",
    "        df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist()\n",
    "        )\n",
    "        de_labels = np.array(\n",
    "        df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist()\n",
    "        )\n",
    "        data_dict[label] = {\n",
    "        'encoder_inputs': en_inputs,\n",
    "        'decoder_inputs': de_inputs,\n",
    "        'decoder_labels': de_labels\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a2ba7-7df7-4e6f-82d0-a5e9c5a9a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_indices=None):\n",
    "    \"\"\"\n",
    "    Shuffle the data randomly (but all of inputs and labels at ones)\n",
    "    \"\"\"\n",
    "    if shuffle_indices is None:\n",
    "        shuffle_indices = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "    return (\n",
    "    en_inputs[shuffle_indices],\n",
    "    de_inputs[shuffle_indices],\n",
    "    de_labels[shuffle_indices]\n",
    "    ), shuffle_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfa886-a9bf-41b0-b161-fd6fbc1646ab",
   "metadata": {},
   "source": [
    "### **Step 10**: Define BLEU Evaluation metric\n",
    "---\n",
    "\n",
    "The source of the ```nmt_bleu.py``` module can be found in this [GIT Repository](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695f8e5-3ec9-4b6c-a726-3fb8b8ad3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from nmt_bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric:\n",
    "    def __init__(self, vocabulary, name='bleu', **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the BLEU score for machine translation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(\n",
    "            vocabulary=self.vocab,\n",
    "            invert=True,\n",
    "            num_oov_indices=1  # Allow one OOV token\n",
    "        )\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score for targets and predictions.\n",
    "        \"\"\"\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  # Get predicted IDs\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)  # Convert to tokens\n",
    "        real_tokens = self.id_to_token_layer(real)  # Convert to tokens\n",
    "        \n",
    "        # Clean and process tokens\n",
    "        def clean_text(tokens):\n",
    "            # Remove padding, strip unwanted tokens\n",
    "            t = tf.strings.strip(\n",
    "                tf.strings.regex_replace(\n",
    "                    tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                    \"eos.*\", \"\"  # Remove everything after \"eos\"\n",
    "                )\n",
    "            )\n",
    "            t = np.char.decode(t.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "            t = [doc if len(doc) > 0 else '[UNK]' for doc in t]\n",
    "            return np.char.split(t).tolist()\n",
    "        \n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        real_tokens = [[r] for r in clean_text(real_tokens)]  # Format for BLEU\n",
    "\n",
    "        # Compute BLEU using the provided `compute_bleu` function\n",
    "        bleu, _, _, _, _, _ = compute_bleu(real_tokens, pred_tokens)\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4091ed-16e8-4087-ae34-6dd2838f08d6",
   "metadata": {},
   "source": [
    "**Explanation**: The *BLEU* metric measures the similarity between predicted translations and ground truth references. It is a widely used evaluation metric for machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b96c0b-7eb4-4599-aee6-883bb53c8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tokens):\n",
    "    \n",
    "\n",
    "    # 3. Strip the string of any extra white spaces\n",
    "    translations_in_bytes = tf.strings.strip(\n",
    "        # 2. Replace everything after the eos token with blank\n",
    "        tf.strings.regex_replace(\n",
    "            # 1. Join all the tokens to one string in each sequence\n",
    "            tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                \"eos.*\", \"\"\n",
    "                ),\n",
    "                )\n",
    "    # Decode the byte stream to a string\n",
    "    translations = np.char.decode(translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "    \n",
    "    # If the string is empty, add a [UNK] token\n",
    "    # Otherwise get a Division by zero error\n",
    "    translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
    "    \n",
    "    # Split the sequences to individual tokens\n",
    "    translations = np.char.split(translations).tolist()\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a0aed-f67f-47e8-9260-49731a63ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, epochs, batch_size):\n",
    "    \"\"\"Evaluate the model on various metrics.\"\"\"\n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        print(f\"Evaluating batch {i + 1}/{n_batches}\", end=\"\\r\")\n",
    "\n",
    "        # Convert inputs to tensors\n",
    "        x = [\n",
    "            tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "        ]\n",
    "        # Convert labels to integer token IDs\n",
    "        y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "        # Evaluate model\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu)\n",
    "\n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5c1bb-5cfa-4d6e-ba97-d34819950c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model to save.\n",
    "        model_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    model.save(model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "    \n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\"Train the model and evaluate on validation/test sets.\"\"\"\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        bleu_log, accuracy_log, loss_log = [], [], []\n",
    "\n",
    "        # Shuffle the training data\n",
    "        (en_inputs_raw, de_inputs_raw, de_labels_raw), shuffle_inds = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training Loop\n",
    "        for i in range(n_train_batches):\n",
    "            print(f\"Training batch {i + 1}/{n_train_batches}\", end=\"\\r\")\n",
    "            x = [\n",
    "                tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "                tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            ]\n",
    "            y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "            # Train on batch\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "            # Track metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            pred_y = model.predict(x)\n",
    "            bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu)\n",
    "\n",
    "        print(f\"\\t(train) loss: {np.mean(loss_log):.4f} - accuracy: {np.mean(accuracy_log):.4f} - bleu: {np.mean(bleu_log):.4f}\")\n",
    "\n",
    "        # Validation after the epoch\n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model,\n",
    "            vectorizer,\n",
    "            data_dict['valid']['encoder_inputs'],\n",
    "            data_dict['valid']['decoder_inputs'],\n",
    "            data_dict['valid']['decoder_labels'],\n",
    "            epochs=1,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        print(f\"\\t(valid) loss: {val_loss:.4f} - accuracy: {val_accuracy:.4f} - bleu: {val_bleu:.4f}\")\n",
    "\n",
    "    # Test evaluation after all epochs\n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        data_dict['test']['encoder_inputs'],\n",
    "        data_dict['test']['decoder_inputs'],\n",
    "        data_dict['test']['decoder_labels'],\n",
    "        epochs=1,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(f\"\\n(test) loss: {test_loss:.4f} - accuracy: {test_accuracy:.4f} - bleu: {test_bleu:.4f}\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    model_path = \"en2de_translatorX.h5\"\n",
    "    save_model(seq2seq_model, model_path)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f6322-2e92-4d23-b051-1049bb4a6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(seq2seq_model, de_vectorizer, train_df, valid_df, test_df,epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1591da2-1245-4f26-a3af-141636c4fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Loads the model from the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path from where the model will be loaded.\n",
    "        \n",
    "    Returns:\n",
    "        Loaded Keras model.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path, custom_objects={'TextVectorization': tf.keras.layers.TextVectorization})\n",
    "    print(f'Model loaded from {model_path}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d92a76-b9e3-4fac-a105-6ed68d1969c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def make_inference(model, en_vectorizer, input_sentence, de_vocab, max_output_length=20):\n",
    "    \"\"\"\n",
    "    Make inference using the trained seq2seq model.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded seq2seq model.\n",
    "        en_vectorizer: TextVectorization layer for the English vocabulary.\n",
    "        input_sentence (str): Input English sentence to translate.\n",
    "        de_vocab (list): List of German vocabulary words.\n",
    "        max_output_length (int): Maximum length of the translated output sequence.\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated German sentence.\n",
    "    \"\"\"\n",
    "    # Vectorize the input sentence\n",
    "    input_vector = tf.convert_to_tensor([input_sentence], dtype=tf.string)\n",
    "    input_token_ids = en_vectorizer(input_vector)\n",
    "\n",
    "    # Initialize the context vector using the encoder\n",
    "    context_vector = model.get_layer(\"Encoder\")(input_token_ids)\n",
    "\n",
    "    # Initialize the decoder input with the start token (e.g., \"sos\")\n",
    "    start_token_index = de_vocab.index(\"sos\")\n",
    "    decoder_input = tf.convert_to_tensor([[start_token_index]], dtype=tf.int32)\n",
    "\n",
    "    output_tokens = []\n",
    "    for _ in range(max_output_length):\n",
    "        # Make a prediction using the decoder\n",
    "        logits = model.get_layer(\"Decoder\")([decoder_input, context_vector])\n",
    "        predicted_token_id = tf.argmax(logits, axis=-1).numpy()[0, -1]\n",
    "\n",
    "        # If the end token is predicted, break the loop\n",
    "        if de_vocab[predicted_token_id] == \"eos\":\n",
    "            break\n",
    "\n",
    "        # Append the predicted token to the output tokens list\n",
    "        output_tokens.append(de_vocab[predicted_token_id])\n",
    "\n",
    "        # Update the decoder input for the next step\n",
    "        decoder_input = tf.convert_to_tensor([[predicted_token_id]], dtype=tf.int32)\n",
    "\n",
    "    # Join the output tokens to form the translated sentence\n",
    "    translated_sentence = \" \".join(output_tokens)\n",
    "    return translated_sentence\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6733d91-4c40-41d2-8300-46020c135e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = load_model_from_file(model_path='en2de_translatorX.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7e9ba-32e4-49d1-a9be-f0e2b6761e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sentence = \"How are you?\"\n",
    "# translated_sentence = make_inference(loaded_model, en_vectorizer, input_sentence, de_vocabulary)\n",
    "# print(f'Translated Sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1f142-8a7d-46c5-bed7-a3de6bfa810b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
