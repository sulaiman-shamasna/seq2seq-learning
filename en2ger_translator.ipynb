{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b803581-d8fb-47f1-9647-15a66f2fad93",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Learning: English-German Translator\n",
    "\n",
    "This notebook illustrates how to develop an English-to-German translation model as a sequence-to-sequence (seq2seq) learning problem using TensorFlow and Keras. This project demonstrates the core components of an encoder-decoder architecture with *BiLSTMs*, handling of text preprocessing, and evaluation using BLEU scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Sequence-to-Sequence Learning Overview\n",
    "\n",
    "Sequence-to-sequence (seq2seq) learning is a type of deep learning model that maps input sequences to output sequences of potentially different lengths. It has wide applications in:\n",
    "\n",
    "- Machine Translation (e.g., English-to-German)\n",
    "- Text Summarization\n",
    "- Chatbots and Conversational Agents\n",
    "\n",
    "Key components:\n",
    "- **Encoder**: Encodes the input sequence into a fixed-length context vector.\n",
    "- **Decoder**: Decodes the context vector to generate the target sequence.\n",
    "\n",
    "\n",
    "***Reference***: [TensorFlow in Action](https://www.google.de/books/edition/TensorFlow_in_Action/JYyKEAAAQBAJ?hl=en&gbpv=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff7a8b5-c2a0-4e27-815b-1ea25619becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Ensure the required dataset is available and extracted\n",
    "def prepare_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the English-German dataset for translation.\n",
    "    Downloads and extracts the data if not already present.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing English and German sentences.\n",
    "    \"\"\"\n",
    "    data_dir = 'data'\n",
    "    zip_path = os.path.join(data_dir, 'deu-eng.zip')\n",
    "    extracted_path = os.path.join(data_dir, 'deu.txt')\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Please download 'deu-eng.zip' from \"\n",
    "            f\"http://www.manythings.org/anki/deu-eng.zip and place it in the {data_dir} folder.\"\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(extracted_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Dataset extracted.\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    df = pd.read_csv(extracted_path, delimiter='\\t', header=None, names=[\"EN\", \"DE\", \"Attribution\"])\n",
    "    df = df[[\"EN\", \"DE\"]]\n",
    "    return df\n",
    "\n",
    "df = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325d0df-c72f-4574-a0ab-1b246f0af276",
   "metadata": {},
   "source": [
    "### Data Setup and Preprocessing\n",
    "---\n",
    "\n",
    "Download [deu-eng dataset](http://www.manythings.org/anki/deu-eng.zip) manually and locate it in the ```\\data``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb765ef7-fb5f-43b4-9ec5-e70383c9a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (277891, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "\n",
    "print('df.shape = {}'.format(df.shape))\n",
    "\n",
    "\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "df = df.iloc[clean_inds]\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb3a317-1fe6-4c0d-b263-c076d10ab47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "random_seed= 4321\n",
    "df = df.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token\n",
    "\n",
    "# Randomly sample 10% examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Randomly sample 10% examples from the remaining randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b17e00-6fb3-4947-b926-400af1763877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \"\"\"\n",
    "    Print the summary stats of the sequence length\n",
    "    \"\"\"\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    \n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    print(seq_length_ser[\n",
    "    (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n",
    "    ].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc714ee9-46ec-4a8c-bc04-75f564f44244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.294025\n",
      "std          2.542850\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39584.000000\n",
      "mean         6.184671\n",
      "std          2.284073\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.332250\n",
      "std          2.536094\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         52.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39227.000000\n",
      "mean         8.253116\n",
      "std          2.231582\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef74d93d-ae69-4e43-ac28-d4e59f2d8024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9228\n",
      "to     8700\n",
      "I      8620\n",
      "the    6766\n",
      "you    6136\n",
      "a      5741\n",
      "is     4141\n",
      "in     2639\n",
      "of     2470\n",
      "was    2380\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2218\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom       9713\n",
      "Ich       7964\n",
      "ist       4735\n",
      "nicht     4616\n",
      "zu        3606\n",
      "Sie       3441\n",
      "du        3132\n",
      "das       2987\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2483\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the vocabulary size above a certain threshold\n",
    "    \"\"\"\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    freq_df = pd.Series(\n",
    "    list(counter.values()),\n",
    "    index=list(counter.keys())\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(freq_df.head(n=10))\n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "    return n_vocab\n",
    "\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46746d73-a993-4ddb-b133-02066e037147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2218\n",
      "DE vocabulary size: 2483\n",
      "\n",
      "\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "print(\"\\n\")\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae46e890-6eff-4310-b2b6-3ede8425e2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.294025\n",
      "std          2.542850\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39584.000000\n",
      "mean         6.184671\n",
      "std          2.284073\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.332250\n",
      "std          2.536094\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         52.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39227.000000\n",
      "mean         8.253116\n",
      "std          2.231582\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \"\"\"\n",
    "    Print the summary stats of the sequence length\n",
    "    \"\"\"\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    \n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    print(seq_length_ser[\n",
    "    (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n",
    "    ].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a33d30-ed5b-4408-a7f1-1c2db38851e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2218\n",
      "DE vocabulary size: 2483\n",
      "\n",
      "\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "print(\"\\n\")\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08484d2-f80f-496e-977c-fade1e87c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('tom'), np.str_('to'), np.str_('i'), np.str_('you'), np.str_('the'), np.str_('a'), np.str_('is'), np.str_('that')]\n",
      "2218\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "en_vectorize_layer = TextVectorization(\n",
    "max_tokens=en_vocab,\n",
    "output_mode='int',\n",
    "output_sequence_length=None\n",
    ")\n",
    "\n",
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "print(len(en_vectorize_layer.get_vocabulary()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9537d61e-f552-47b9-9150-9a2a3f068523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Input data: \n",
      "[b'run' b'how are you' b'ectoplasmic residue']\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[3 0 0 0 0 0 0 0 0 0]\n",
      " [5 7 2 0 0 0 0 0 0 0]\n",
      " [6 4 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Vocabulary size and sequence length\n",
    "en_vocab = 1000\n",
    "\n",
    "# Define the TextVectorization layer\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=10  # Adjust sequence length as needed\n",
    ")\n",
    "\n",
    "# Adapt the TextVectorization layer to a sample dataset\n",
    "sample_text = [\"run\", \"how are you\", \"ectoplasmic residue\"]\n",
    "en_vectorize_layer.adapt(sample_text)\n",
    "\n",
    "# Create a toy model with the TextVectorization layer\n",
    "toy_model = Sequential([en_vectorize_layer])\n",
    "\n",
    "# Input data (ensure it's a TensorFlow tensor of strings)\n",
    "input_data = tf.constant([\"run\", \"how are you\", \"ectoplasmic residue\"])\n",
    "\n",
    "# Predict token IDs using the model\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "# Display results\n",
    "print(\"Input data: \\n{}\\n\".format(input_data.numpy()))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddd86f4-b30a-4f1a-9822-00ca49b9d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(\n",
    "    corpus, n_vocab=100, max_length=None, return_vocabulary=True, name=None\n",
    "    ):\n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "                                                        max_tokens=n_vocab+2,\n",
    "                                                        output_mode='int',\n",
    "                                                        output_sequence_length=max_length,\n",
    "                                                        )\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    if not return_vocabulary:\n",
    "        return tf.keras.models.Model(\n",
    "    inputs=inp, outputs=vectorized_out, name=name\n",
    "    )\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f54366e4-512b-4637-96fd-78fd3700d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\n",
    "max_length=en_seq_length, name='en_vectorizer'\n",
    ")\n",
    "\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n",
    "max_length=de_seq_length-1, name='de_vectorizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68de6e87-380d-4a50-b0fc-3fccde705aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 100\n",
    "\n",
    "# The input is (None,1) shaped and accepts an array of strings\n",
    "inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "# Vectorize the data (assign token IDs)\n",
    "vectorized_out = en_vectorizer(inp)\n",
    "\n",
    "# Define an embedding layer to convert IDs to word vectors\n",
    "emb_layer = tf.keras.layers.Embedding(\n",
    "input_dim=n_vocab+2, output_dim=128, mask_zero=True, name='e_embedding')\n",
    "\n",
    "# Get the embeddings of the token IDs\n",
    "emb_out = emb_layer(vectorized_out)\n",
    "\n",
    "gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128))\n",
    "gru_out = gru_layer(emb_out)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366a3dac-5185-4dd8-9720-2fb91ca4a5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ e_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ en_vectorizer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ e_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ e_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">128,256</span> │ en_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ en_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ e_bidirectional_gru │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,144</span> │ e_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ e_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ en_vectorizer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ e_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ e_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m128,256\u001b[0m │ en_vectorizer[\u001b[38;5;34m1\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ en_vectorizer[\u001b[38;5;34m1\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ e_bidirectional_gru │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m198,144\u001b[0m │ e_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,400</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m326,400\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">326,400</span> (1.25 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m326,400\u001b[0m (1.25 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_encoder(n_vocab, vectorizer):\n",
    "    \"\"\"\n",
    "    Define the encoder of the seq2seq model\n",
    "    \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    emb_layer = tf.keras.layers.Embedding(\n",
    "    n_vocab+2, 128, mask_zero=True, name='e_embedding'\n",
    "    )\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    gru_layer = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.GRU(128, name='e_gru'),\n",
    "    name='e_bidirectional_gru'\n",
    "    )\n",
    "\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    encoder = tf.keras.models.Model(\n",
    "    inputs=inp, outputs=gru_out, name='encoder'\n",
    "    )\n",
    "    return encoder\n",
    "\n",
    "encoder = get_encoder(en_vocab, en_vectorizer)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a453331-d11a-4a43-85da-29ec13f02417",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
    "d_init_state = encoder(e_inp)\n",
    "\n",
    "d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "vectorized_out = de_vectorizer(inp)\n",
    "\n",
    "emb_layer = tf.keras.layers.Embedding(\n",
    "input_dim=n_vocab+2, output_dim=128, mask_zero=True, name='d_embedding'\n",
    ")\n",
    "emb_out = emb_layer(vectorized_out)\n",
    "\n",
    "gru_layer = tf.keras.layers.GRU(256, return_sequences=True)\n",
    "gru_out = gru_layer(emb_out, initial_state=d_init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5707b356-9a4b-411a-9bed-00ea6af3f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
    "    \"\"\"\n",
    "    Define the final encoder-decoder model\n",
    "    \"\"\"\n",
    "    \n",
    "    e_inp = tf.keras.Input(\n",
    "    shape=(1,), dtype=tf.string, name='e_input_final'\n",
    "    )\n",
    "    d_init_state = encoder(e_inp)\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    d_vectorized_out = vectorizer(d_inp)\n",
    "    d_emb_layer = tf.keras.layers.Embedding(\n",
    "    n_vocab+2, 128, mask_zero=True, name='d_embedding'\n",
    "    )\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    d_gru_layer = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, name='d_gru'\n",
    "    )\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
    "    d_dense_layer_1 = tf.keras.layers.Dense(\n",
    "    512, activation='relu', name='d_dense_1'\n",
    "    )\n",
    "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
    "    d_dense_layer_final = tf.keras.layers.Dense(\n",
    "    n_vocab+2, activation='softmax', name='d_dense_final'\n",
    "    )\n",
    "    d_final_out = d_dense_layer_final(d_dense1_out)\n",
    "    seq2seq = tf.keras.models.Model(\n",
    "    inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq'\n",
    "    )\n",
    "    return seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4a3f52-4347-4968-99fe-b2f3301638b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\n",
    "max_length=en_seq_length, name='e_vectorizer'\n",
    ")\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n",
    "max_length=de_seq_length-1, name='d_vectorizer'\n",
    ")\n",
    "# Define the final model\n",
    "encoder = get_encoder(n_vocab=en_vocab, vectorizer=en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(\n",
    "n_vocab=de_vocab, encoder=encoder, vectorizer=de_vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3060d46e-39a3-431a-8f28-e864689edba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure [UNK] is at the beginning of the vocabulary\n",
    "en_vocabulary = [v for v in en_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "en_vocabulary = ['[UNK]'] + en_vocabulary  # Add [UNK] at the start\n",
    "\n",
    "de_vocabulary = [v for v in de_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "de_vocabulary = ['[UNK]'] + de_vocabulary  # Add [UNK] at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5edab6bc-638f-458c-9ccd-408ca7f09547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "final_model.compile(\n",
    "loss='sparse_categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ac0f92e-b83c-416c-930f-3591b3c97cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"final_seq2seq\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"final_seq2seq\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ d_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_vectorizer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ d_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ e_input_final       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">318,080</span> │ d_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">326,400</span> │ e_input_final[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ d_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ d_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_dense_final       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2485</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,274,805</span> │ d_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ d_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_vectorizer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ d_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ e_input_final       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m318,080\u001b[0m │ d_vectorizer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m326,400\u001b[0m │ e_input_final[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_gru (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ d_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_dense_1 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ d_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ d_dense_final       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2485\u001b[0m)  │  \u001b[38;5;34m1,274,805\u001b[0m │ d_dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,347,317</span> (8.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,347,317\u001b[0m (8.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,347,317</span> (8.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,347,317\u001b[0m (8.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b2cda29-617e-4b62-afb5-b28fb6c1840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    Create a data dictionary from the dataframes containing data\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for label, df in zip(\n",
    "        ['train', 'valid', 'test'], [train_df, valid_df, test_df]\n",
    "        ):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(\n",
    "        df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist()\n",
    "        )\n",
    "        de_labels = np.array(\n",
    "        df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist()\n",
    "        )\n",
    "        data_dict[label] = {\n",
    "        'encoder_inputs': en_inputs,\n",
    "        'decoder_inputs': de_inputs,\n",
    "        'decoder_labels': de_labels\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877a2ba7-7df7-4e6f-82d0-a5e9c5a9a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_indices=None):\n",
    "    \"\"\"\n",
    "    Shuffle the data randomly (but all of inputs and labels at ones)\n",
    "    \"\"\"\n",
    "    if shuffle_indices is None:\n",
    "        shuffle_indices = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "    return (\n",
    "    en_inputs[shuffle_indices],\n",
    "    de_inputs[shuffle_indices],\n",
    "    de_labels[shuffle_indices]\n",
    "    ), shuffle_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0695f8e5-3ec9-4b6c-a726-3fb8b8ad3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "class BLEUMetric:\n",
    "    def __init__(self, vocabulary, name='bleu', **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the BLEU score for machine translation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(\n",
    "            vocabulary=self.vocab,\n",
    "            invert=True,\n",
    "            num_oov_indices=1  # Allow one OOV token\n",
    "        )\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score for targets and predictions.\n",
    "        \"\"\"\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  # Get predicted IDs\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)  # Convert to tokens\n",
    "        real_tokens = self.id_to_token_layer(real)  # Convert to tokens\n",
    "        \n",
    "        # Clean and process tokens\n",
    "        def clean_text(tokens):\n",
    "            # Remove padding, strip unwanted tokens\n",
    "            t = tf.strings.strip(\n",
    "                tf.strings.regex_replace(\n",
    "                    tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                    \"eos.*\", \"\"  # Remove everything after \"eos\"\n",
    "                )\n",
    "            )\n",
    "            t = np.char.decode(t.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "            t = [doc if len(doc) > 0 else '[UNK]' for doc in t]\n",
    "            return np.char.split(t).tolist()\n",
    "        \n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        real_tokens = [[r] for r in clean_text(real_tokens)]  # Format for BLEU\n",
    "\n",
    "        # Compute BLEU using the provided `compute_bleu` function\n",
    "        bleu, _, _, _, _, _ = compute_bleu(real_tokens, pred_tokens)\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79a810ad-1e04-48b8-a6c3-59797f9da082",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_bleu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m translation \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmÃssen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merfahrung\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbringen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msie\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwohnen\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m reference \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mals\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmÃssen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmÃssen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merfahrung\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbringen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msie\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwohnen\u001b[39m\u001b[38;5;124m'\u001b[39m]]]\n\u001b[1;32m----> 4\u001b[0m bleu1, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_bleu\u001b[49m(reference, translation)\n\u001b[0;32m      6\u001b[0m translation \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meinmal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmÃssen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merfahrung\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbringen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msie\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwohnen\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      7\u001b[0m reference \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mals\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmÃssen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmÃssen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merfahrung\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbringen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msie\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwohnen\u001b[39m\u001b[38;5;124m'\u001b[39m]]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_bleu' is not defined"
     ]
    }
   ],
   "source": [
    "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "\n",
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "print(\"BLEU score with longer correctly predicte phrases: {}\".format(bleu1))\n",
    "print(\"BLEU score without longer correctly predicte phrases: {}\".format(bleu2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b96c0b-7eb4-4599-aee6-883bb53c8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tokens):\n",
    "    \n",
    "\n",
    "    # 3. Strip the string of any extra white spaces\n",
    "    translations_in_bytes = tf.strings.strip(\n",
    "        # 2. Replace everything after the eos token with blank\n",
    "        tf.strings.regex_replace(\n",
    "            # 1. Join all the tokens to one string in each sequence\n",
    "            tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                \"eos.*\", \"\"\n",
    "                ),\n",
    "                )\n",
    "    # Decode the byte stream to a string\n",
    "    translations = np.char.decode(translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "    \n",
    "    # If the string is empty, add a [UNK] token\n",
    "    # Otherwise get a Division by zero error\n",
    "    translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
    "    \n",
    "    # Split the sequences to individual tokens\n",
    "    translations = np.char.split(translations).tolist()\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dfee0e-f06f-4ec8-84ef-b9d0af647b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung','bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075764a2-00fc-420a-998b-bd6eec33872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung','bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a414479-f287-4f52-83c2-d1ba81d5b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "print(\"BLEU score with longer correctly predict phrases: {}\".format(bleu1))\n",
    "print(\"BLEU score without longer correctly predict phrases:{}\".format(bleu2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a0aed-f67f-47e8-9260-49731a63ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, epochs, batch_size):\n",
    "    \"\"\"Evaluate the model on various metrics.\"\"\"\n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        print(f\"Evaluating batch {i + 1}/{n_batches}\", end=\"\\r\")\n",
    "\n",
    "        # Convert inputs to tensors\n",
    "        x = [\n",
    "            tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "        ]\n",
    "        # Convert labels to integer token IDs\n",
    "        y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "        # Evaluate model\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu)\n",
    "\n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5c1bb-5cfa-4d6e-ba97-d34819950c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model to save.\n",
    "        model_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    model.save(model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "    \n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\"Train the model and evaluate on validation/test sets.\"\"\"\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        bleu_log, accuracy_log, loss_log = [], [], []\n",
    "\n",
    "        # Shuffle the training data\n",
    "        (en_inputs_raw, de_inputs_raw, de_labels_raw), shuffle_inds = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training Loop\n",
    "        for i in range(n_train_batches):\n",
    "            print(f\"Training batch {i + 1}/{n_train_batches}\", end=\"\\r\")\n",
    "            x = [\n",
    "                tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "                tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            ]\n",
    "            y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "            # Train on batch\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "            # Track metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            pred_y = model.predict(x)\n",
    "            bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu)\n",
    "\n",
    "        print(f\"\\t(train) loss: {np.mean(loss_log):.4f} - accuracy: {np.mean(accuracy_log):.4f} - bleu: {np.mean(bleu_log):.4f}\")\n",
    "\n",
    "        # Validation after the epoch\n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model,\n",
    "            vectorizer,\n",
    "            data_dict['valid']['encoder_inputs'],\n",
    "            data_dict['valid']['decoder_inputs'],\n",
    "            data_dict['valid']['decoder_labels'],\n",
    "            epochs=1,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        print(f\"\\t(valid) loss: {val_loss:.4f} - accuracy: {val_accuracy:.4f} - bleu: {val_bleu:.4f}\")\n",
    "\n",
    "    # Test evaluation after all epochs\n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        data_dict['test']['encoder_inputs'],\n",
    "        data_dict['test']['decoder_inputs'],\n",
    "        data_dict['test']['decoder_labels'],\n",
    "        epochs=1,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(f\"\\n(test) loss: {test_loss:.4f} - accuracy: {test_accuracy:.4f} - bleu: {test_bleu:.4f}\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    model_path = \"en2de_translatorX.h5\"\n",
    "    save_model(final_model, model_path)\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f6322-2e92-4d23-b051-1049bb4a6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(final_model, de_vectorizer, train_df, valid_df, test_df,epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1591da2-1245-4f26-a3af-141636c4fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Loads the model from the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path from where the model will be loaded.\n",
    "        \n",
    "    Returns:\n",
    "        Loaded Keras model.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path, custom_objects={'TextVectorization': tf.keras.layers.TextVectorization})\n",
    "    print(f'Model loaded from {model_path}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d92a76-b9e3-4fac-a105-6ed68d1969c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(model, en_vectorizer, input_sentence, de_vocab, max_output_length=20):\n",
    "    \"\"\"\n",
    "    Make inference using the trained seq2seq model.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded seq2seq model.\n",
    "        en_vectorizer: TextVectorization layer for the English vocabulary.\n",
    "        input_sentence (str): Input English sentence to translate.\n",
    "        de_vocab (list): List of German vocabulary words.\n",
    "        max_output_length (int): Maximum length of the translated output sequence.\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated German sentence.\n",
    "    \"\"\"\n",
    "    # Vectorize the input sentence\n",
    "    input_vector = tf.convert_to_tensor([input_sentence], dtype=tf.string)\n",
    "    input_token_ids = en_vectorizer(input_vector)\n",
    "\n",
    "    # Initialize the context vector using the encoder\n",
    "    context_vector = model.get_layer(\"Encoder\")(input_token_ids)\n",
    "\n",
    "    # Initialize the decoder input with the start token (e.g., \"sos\")\n",
    "    start_token_index = de_vocab.index(\"sos\")\n",
    "    decoder_input = tf.convert_to_tensor([[start_token_index]], dtype=tf.int32)\n",
    "\n",
    "    output_tokens = []\n",
    "    for _ in range(max_output_length):\n",
    "        # Make a prediction using the decoder\n",
    "        logits = model.get_layer(\"Decoder\")([decoder_input, context_vector])\n",
    "        predicted_token_id = tf.argmax(logits, axis=-1).numpy()[0, -1]\n",
    "\n",
    "        # If the end token is predicted, break the loop\n",
    "        if de_vocab[predicted_token_id] == \"eos\":\n",
    "            break\n",
    "\n",
    "        # Append the predicted token to the output tokens list\n",
    "        output_tokens.append(de_vocab[predicted_token_id])\n",
    "\n",
    "        # Update the decoder input for the next step\n",
    "        decoder_input = tf.convert_to_tensor([[predicted_token_id]], dtype=tf.int32)\n",
    "\n",
    "    # Join the output tokens to form the translated sentence\n",
    "    translated_sentence = \" \".join(output_tokens)\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6733d91-4c40-41d2-8300-46020c135e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model_from_file(model_path='en2de_translatorX.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7e9ba-32e4-49d1-a9be-f0e2b6761e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"How are you?\"\n",
    "translated_sentence = make_inference(loaded_model, en_vectorizer, input_sentence, de_vocabulary)\n",
    "print(f'Translated Sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1f142-8a7d-46c5-bed7-a3de6bfa810b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
