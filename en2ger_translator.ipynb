{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b803581-d8fb-47f1-9647-15a66f2fad93",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Learning: English-German Translator\n",
    "\n",
    "This notebook illustrates how to develop an English-to-German translation model as a sequence-to-sequence (seq2seq) learning problem using TensorFlow and Keras. This project demonstrates the core components of an encoder-decoder architecture with *BiLSTMs*, handling of text preprocessing, and evaluation using BLEU scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Sequence-to-Sequence Learning Overview\n",
    "\n",
    "Sequence-to-sequence (seq2seq) learning is a type of deep learning model that maps input sequences to output sequences of potentially different lengths. It has wide applications in:\n",
    "\n",
    "- Machine Translation (e.g., English-to-German)\n",
    "- Text Summarization\n",
    "- Chatbots and Conversational Agents\n",
    "\n",
    "Key components:\n",
    "- **Encoder**: Encodes the input sequence into a fixed-length context vector.\n",
    "- **Decoder**: Decodes the context vector to generate the target sequence.\n",
    "\n",
    "\n",
    "***Reference***: [TensorFlow in Action](https://www.google.de/books/edition/TensorFlow_in_Action/JYyKEAAAQBAJ?hl=en&gbpv=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab96504-1e7d-4489-8c35-a040e1054016",
   "metadata": {},
   "source": [
    "### **Step 1**: Download and Extract the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff7a8b5-c2a0-4e27-815b-1ea25619becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Ensure the required dataset is available and extracted\n",
    "def prepare_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the English-German dataset for translation.\n",
    "    Downloads and extracts the data if not already present.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing English and German sentences.\n",
    "    \"\"\"\n",
    "    data_dir = 'data'\n",
    "    zip_path = os.path.join(data_dir, 'deu-eng.zip')\n",
    "    extracted_path = os.path.join(data_dir, 'deu.txt')\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Please download 'deu-eng.zip' from \"\n",
    "            f\"http://www.manythings.org/anki/deu-eng.zip and place it in the {data_dir} folder.\"\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(extracted_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Dataset extracted.\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    df = pd.read_csv(extracted_path, delimiter='\\t', header=None, names=[\"EN\", \"DE\", \"Attribution\"])\n",
    "    df = df[[\"EN\", \"DE\"]]\n",
    "    return df\n",
    "\n",
    "df = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55574daa-31e4-4f56-a0bc-ddb2c91073e8",
   "metadata": {},
   "source": [
    "**Explanation**: This step ensures the required dataset is extracted for processing. If the dataset is not present in the expected location, it prompts the user to download it manually.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325d0df-c72f-4574-a0ab-1b246f0af276",
   "metadata": {},
   "source": [
    "### **Step 2**: Data Setup and Preprocessing\n",
    "---\n",
    "\n",
    "Download [deu-eng dataset](http://www.manythings.org/anki/deu-eng.zip) manually and locate it in the ```\\data``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb765ef7-fb5f-43b4-9ec5-e70383c9a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (277891, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "\n",
    "print('df.shape = {}'.format(df.shape))\n",
    "\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "df = df.iloc[clean_inds]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01103c-7de1-411c-911e-2eb575762762",
   "metadata": {},
   "source": [
    "**Explanation**: In this step, we clean the data by removing rows with unwanted characters and retain only English and German sentence pairs for the translation task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0862044-6ebf-459b-86eb-072df1b974f5",
   "metadata": {},
   "source": [
    "### **Step 3**: Random Sampling and Token Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb3a317-1fe6-4c0d-b263-c076d10ab47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "random_seed= 4321\n",
    "df = df.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token\n",
    "\n",
    "# Randomly sample 10% examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Randomly sample 10% examples from the remaining randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f41bf-32cd-4de5-b52a-96a666166eee",
   "metadata": {},
   "source": [
    "**Explanation**: This step prepares a subset of ```50,000``` sentences for the task, appends start (```sos```) and end (```eos```) tokens to each German sentence, and splits the data into training, validation, and test sets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df912329-842f-429c-90d0-ee5842b112da",
   "metadata": {},
   "source": [
    "### **Step 4**: Analyze Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b17e00-6fb3-4947-b926-400af1763877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"English corpus\")\\nprint(\\'=\\'*50)\\nprint_sequence_length(train_df[\"EN\"])\\nprint(\"\\nGerman corpus\")\\nprint(\\'=\\'*50)\\n\\nprint_sequence_length(train_df[\"DE\"])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \"\"\"\n",
    "    Print the summary stats of the sequence length\n",
    "    \"\"\"\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    \n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    print(seq_length_ser[\n",
    "    (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n",
    "    ].describe())\n",
    "\n",
    "\"\"\"\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "\n",
    "print_sequence_length(train_df[\"DE\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0efa8-3532-40f6-afef-135d3019df28",
   "metadata": {},
   "source": [
    "**Explanation**: Here, we analyze the sequence lengths for both corpora. The results help determine appropriate sequence lengths for the encoder and decoder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8a47d-6660-46b0-ab63-484f1c966d50",
   "metadata": {},
   "source": [
    "### **Step 5**: Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef74d93d-ae69-4e43-ac28-d4e59f2d8024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom    9228\n",
      "to     8700\n",
      "I      8620\n",
      "the    6766\n",
      "you    6136\n",
      "a      5741\n",
      "is     4141\n",
      "in     2639\n",
      "of     2470\n",
      "was    2380\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2218\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom       9713\n",
      "Ich       7964\n",
      "ist       4735\n",
      "nicht     4616\n",
      "zu        3606\n",
      "Sie       3441\n",
      "du        3132\n",
      "das       2987\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2483\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the vocabulary size above a certain threshold\n",
    "    \"\"\"\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    freq_df = pd.Series(\n",
    "    list(counter.values()),\n",
    "    index=list(counter.keys())\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(freq_df.head(n=10))\n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "    return n_vocab\n",
    "\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05abad-518a-4a73-beaa-66fd33cfeeb9",
   "metadata": {},
   "source": [
    "**Explanation**: We compute the vocabulary size for both languages, focusing on words that appear at least 10 times. This informs the TextVectorization step later in the pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46746d73-a993-4ddb-b133-02066e037147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2218\n",
      "DE vocabulary size: 2483\n",
      "\n",
      "\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13f8f0-fbbf-4c23-9673-f5839cbb2262",
   "metadata": {},
   "source": [
    "## *Writing an English-German seq2seq Machine Translator*\n",
    "---\n",
    "\n",
    "Machine translation involves transforming text from one language to another. In this notebook, I focus on creating an *English-to-German* translator using a *sequence-to-sequence* (*seq2seq*) deep learning model.\n",
    "\n",
    "**Seq2Seq Model Architecture**. The seq2seq model consists of two primary components:\n",
    "- *Encoder:*\n",
    "    - Processes the input (English) text and generates a fixed-length context vector, also known as a \"*[thought vector](https://wiki.pathmind.com/thought-vectors)*.\"\n",
    "    - Encodes the input sequence into a hidden representation that summarizes its semantic and syntactic content.\n",
    "- *Decoder:*\n",
    "  - Takes the context vector produced by the encoder as input.\n",
    "  - Decodes it to produce the output sequence (German text).\n",
    "\n",
    "Both the encoder and decoder are *recurrent neural networks* (*RNNs*), making them suitable for handling sequential data of arbitrary lengths.\n",
    "\n",
    "**Key Challenges in Seq2Seq Learning**:\n",
    "- *Variable Lengths*: The input and output sequences often differ in length. For example, the number of words in a translation might be fewer or greater than the source text.\n",
    "- *Mapping Arbitrary Sequences*: The model must map an input sequence of arbitrary length to an output sequence of arbitrary length while preserving contextual meaning.\n",
    "\n",
    "![encoder-decoder-machine-translation](plots/enc-dec.svg)\n",
    "\n",
    "The *encoder* in our sequence-to-sequence model is built using a *Gated Recurrent Unit* (*GRU*), a type of *RNN*. Its role is to process the input sequence and produce a fixed-size output that summarizes the sequence's information. The *GRU* processes each element of the input sequence step by step, updating its hidden state based on the current input and the previous hidden state. After processing the entire sequence, the final hidden state of the *GRU* serves as the context vector, which encapsulates the semantic and syntactic information of the input.\n",
    "\n",
    "The *decoder*, which is also based on a *GRU*, further processes the context vector to generate the target sequence. In addition to the *GRU*, the *decoder* incorporates *Dense layers*, which play a critical role in producing the final output. The Dense layers map the *GRU's* outputs to the target vocabulary, generating a probability distribution over possible words at each time step. A key feature of the *decoder* is that the weights of the Dense layers are shared across time steps. This means that, similar to how the *GRU* updates and reuses the same weights for each input in the sequence, the Dense layers also reuse their weights for predicting each word in the output sequence. This approach ensures consistency and efficiency in processing sequential data.\n",
    "\n",
    "![encoder-decoder-machine-translation](plots/gru.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcedb2-3687-4251-b8a9-43498ce0caaf",
   "metadata": {},
   "source": [
    "### **Step 6**: Text Vectorization\n",
    "\n",
    "The TextVectorization layer takes in a string, tokenizes it, and converts the tokens to *IDs* by means of a vocabulary (or dictionary) lookup. It takes a list of strings (or an array of strings) as the input, where each string can be a *word/phrase/sentence* (and so on). Then it learns the vocabulary from that corpus. Finally, the layer can be used to convert a list of strings to a tensor that contains a sequence of token IDs for each string in the list provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddd86f4-b30a-4f1a-9822-00ca49b9d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def get_vectorizer(\n",
    "    corpus, n_vocab=100, max_length=None, return_vocabulary=True, name=None\n",
    "    ):\n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "                                                        max_tokens=n_vocab+2,\n",
    "                                                        output_mode='int',\n",
    "                                                        output_sequence_length=max_length,\n",
    "                                                        )\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    if not return_vocabulary:\n",
    "        return tf.keras.models.Model(\n",
    "    inputs=inp, outputs=vectorized_out, name=name\n",
    "    )\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\n",
    "max_length=en_seq_length, name='en_vectorizer'\n",
    ")\n",
    "\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n",
    "max_length=de_seq_length-1, name='de_vectorizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb05bda-4726-43bc-abf7-23a53e01f67e",
   "metadata": {},
   "source": [
    "**Explanation**: Text vectorization converts sentences into sequences of integer token IDs, which are compatible with neural network input layers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34688d65-8a92-4626-9059-8ef8ca56230c",
   "metadata": {},
   "source": [
    "### **Step 7**: Build the Encoder and the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366a3dac-5185-4dd8-9720-2fb91ca4a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_encoder(n_vocab, vectorizer):\n",
    "    \"\"\"\n",
    "    Build the encoder for the seq2seq model.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # Text vectorization\n",
    "    vectorized_output = vectorizer(encoder_input)\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=n_vocab + 2, \n",
    "        output_dim=128, \n",
    "        mask_zero=True, \n",
    "        name='encoder_embedding'\n",
    "    )\n",
    "    embedded_output = embedding_layer(vectorized_output)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    bidirectional_gru = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, name='encoder_gru'), \n",
    "        name='encoder_bidirectional_gru'\n",
    "    )\n",
    "    encoder_output = bidirectional_gru(embedded_output)\n",
    "    \n",
    "    # Define encoder model\n",
    "    encoder_model = tf.keras.models.Model(\n",
    "        inputs=encoder_input, \n",
    "        outputs=encoder_output, \n",
    "        name='encoder'\n",
    "    )\n",
    "    return encoder_model\n",
    "\n",
    "\n",
    "def build_decoder(n_vocab, encoder, vectorizer):\n",
    "    \"\"\"\n",
    "    Build the decoder for the seq2seq model.\n",
    "    \"\"\"\n",
    "    # Encoder input for initializing the decoder's GRU state\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input_final')\n",
    "    decoder_initial_state = encoder(encoder_input)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='decoder_input')\n",
    "    \n",
    "    # Text vectorization for decoder input\n",
    "    vectorized_output = vectorizer(decoder_input)\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=n_vocab + 2, \n",
    "        output_dim=128, \n",
    "        mask_zero=True, \n",
    "        name='decoder_embedding'\n",
    "    )\n",
    "    embedded_output = embedding_layer(vectorized_output)\n",
    "    \n",
    "    # GRU layer\n",
    "    gru_layer = tf.keras.layers.GRU(\n",
    "        256, \n",
    "        return_sequences=True, \n",
    "        name='decoder_gru'\n",
    "    )\n",
    "    gru_output = gru_layer(embedded_output, initial_state=decoder_initial_state)\n",
    "    \n",
    "    # Dense layers for output\n",
    "    dense_layer_1 = tf.keras.layers.Dense(\n",
    "        512, \n",
    "        activation='relu', \n",
    "        name='decoder_dense_1'\n",
    "    )\n",
    "    dense_output_1 = dense_layer_1(gru_output)\n",
    "    \n",
    "    dense_layer_final = tf.keras.layers.Dense(\n",
    "        n_vocab + 2, \n",
    "        activation='softmax', \n",
    "        name='decoder_dense_final'\n",
    "    )\n",
    "    decoder_output = dense_layer_final(dense_output_1)\n",
    "    \n",
    "    # Define decoder model\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        inputs=[encoder_input, decoder_input], \n",
    "        outputs=decoder_output, \n",
    "        name='decoder'\n",
    "    )\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f710ebd-ceb8-43d0-bc83-782583211db6",
   "metadata": {},
   "source": [
    "**Explanation**: The *encoder* processes the input sequence using text vectorization, embeddings, and a bidirectional GRU layer. It produces a context vector representing the input sequence. The *decoder*, on the other hand takes the encoder's context vector and generates output sequences. It uses a GRU layer for decoding and dense layers for token probability distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e78fc-ff3b-4ec6-8d9c-cc883a7f8fcb",
   "metadata": {},
   "source": [
    "#### *Bidirectional RNN: Reading text forward and backward*\n",
    "---\n",
    "The standard *RNN* reads the text forward, one time step at a time, and outputs a sequence of outputs. *Bidirectional RNNs*, as the name suggests, not only read the text forward, but read it backward. This means that bidirectional *RNNs* have twosequences of outputs. Then these two sequences are combined using a combination strategy (e.g., concatenation) to produce the final output. *Bidirectional RNNs* typically outperform standard RNNs because they understand relationships in text both forward and backward, as shown in the following figure.\n",
    "\n",
    "![rnn-bi-rnn](plots/rnn-bi-rnn.svg)\n",
    "\n",
    "Why does reading text backward help? There are some languages that are read backward (e.g., Arabic, Hebrew). Unless the text is specifically processed to account for this writing style, a standard RNN would have a very difficult time understanding the language. By having a *bidirectional RNN*, you are removing the model's dependency on a language to always be left to right or right to left. If we consider the English language, there can be instances where it's impossible to infer a relationship going only forward. Consider the two sentences John went toward the bank on Clarence Street. John went towards the bank of the river. Since the two sentences are identical up to the word \"bank,\" it is not possible to know if the bank is referring to the financial institution or a river bank until you read the rest. For a bidirectional RNN, this is trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d71c1-f56d-4c29-93f7-ef2676d7fbd6",
   "metadata": {},
   "source": [
    "### **Step 8**: Build and Compile the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5707b356-9a4b-411a-9bed-00ea6af3f0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ en_vectorizer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">284,160</span> │ en_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ en_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bidirectio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,144</span> │ encoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ en_vectorizer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m284,160\u001b[0m │ en_vectorizer[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ en_vectorizer[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bidirectio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m198,144\u001b[0m │ encoder_embeddin… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">482,304</span> (1.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m482,304\u001b[0m (1.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">482,304</span> (1.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m482,304\u001b[0m (1.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ de_vectorizer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_input_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">318,080</span> │ de_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">482,304</span> │ encoder_input_fi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ decoder_embeddin… │\n",
       "│                     │                   │            │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ decoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2485</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,274,805</span> │ decoder_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ de_vectorizer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_input_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m318,080\u001b[0m │ de_vectorizer[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m482,304\u001b[0m │ encoder_input_fi… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_gru (\u001b[38;5;33mGRU\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ decoder_embeddin… │\n",
       "│                     │                   │            │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ decoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2485\u001b[0m)  │  \u001b[38;5;34m1,274,805\u001b[0m │ decoder_dense_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"seq2seq_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2485</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> │ encoder_input_fi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2485\u001b[0m)  │  \u001b[38;5;34m2,503,221\u001b[0m │ encoder_input_fi… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_seq2seq_model(encoder, decoder):\n",
    "    \"\"\"\n",
    "    Build the final seq2seq model using the pre-built encoder and decoder.\n",
    "    \"\"\"\n",
    "    # Encoder input\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input_final')\n",
    "    encoder_output = encoder(encoder_input)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='decoder_input')\n",
    "\n",
    "    # Use the decoder model\n",
    "    decoder_output = decoder([encoder_input, decoder_input])\n",
    "\n",
    "    # Define the seq2seq model\n",
    "    seq2seq_model = tf.keras.models.Model(\n",
    "        inputs=[encoder_input, decoder_input], \n",
    "        outputs=decoder_output, \n",
    "        name='seq2seq_model'\n",
    "    )\n",
    "    return seq2seq_model\n",
    "\n",
    "\n",
    "# Usage\n",
    "# Assuming `en_vocab`, `en_vectorizer`, and `de_vectorizer` are defined\n",
    "encoder = build_encoder(en_vocab, en_vectorizer)\n",
    "decoder = build_decoder(de_vocab, encoder, de_vectorizer)\n",
    "seq2seq_model = build_seq2seq_model(encoder, decoder)\n",
    "\n",
    "# Summaries\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3060d46e-39a3-431a-8f28-e864689edba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure [UNK] is at the beginning of the vocabulary\n",
    "en_vocabulary = [v for v in en_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "en_vocabulary = ['[UNK]'] + en_vocabulary  # Add [UNK] at the start\n",
    "\n",
    "de_vocabulary = [v for v in de_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "de_vocabulary = ['[UNK]'] + de_vocabulary  # Add [UNK] at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5edab6bc-638f-458c-9ccd-408ca7f09547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "seq2seq_model.compile(\n",
    "loss='sparse_categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ac0f92e-b83c-416c-930f-3591b3c97cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"seq2seq_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2485</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> │ encoder_input_fi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2485\u001b[0m)  │  \u001b[38;5;34m2,503,221\u001b[0m │ encoder_input_fi… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3253e1c-e023-4104-bd12-a02640026ed0",
   "metadata": {},
   "source": [
    "**Explanation**: The seq2seq model combines the encoder and decoder into a unified framework for training and inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4f3c3-da81-4ef4-b29c-5dfe6842efc5",
   "metadata": {},
   "source": [
    "#### *Detailed Explanation of Seq2Seq Encoder and Decoder Construction*\n",
    "---\n",
    "The Seq2Seq model is designed to process sequences of text, such as translating sentences from one language to another. It consists of two key components: the encoder and the decoder. While the encoder compresses the input sequence into a context vector, the decoder expands that context vector to generate the output sequence. Let’s break down the process based on the provided implementation.\n",
    "\n",
    "**Encoder: Compressing the Input Sequence**. \n",
    "\n",
    "The encoder processes the input text sequence and produces a context vector (a fixed-size representation of the input). Here's how it's implemented:\n",
    "1. *Input Layer*. An input layer ```encoder_input``` accepts sequences of text strings with a shape of ```(1,)```. The input type is ```tf.string```, enabling flexible handling of textual data.\n",
    "2. *Text Vectorization*. The input text is processed through the vectorizer to tokenize and convert it into numerical IDs.\n",
    "3. *Embedding Layer*. These token IDs are mapped into dense vectors using an ```Embedding``` layer with a vocabulary ```size n_vocab + 2```, an output dimension of ```128```, and ```mask_zero=True``` to handle padding.\n",
    "4. *Bidirectional GRU*. A bidirectional GRU layer processes the embeddings, combining information from both forward and backward passes. This produces a robust context vector.\n",
    "5. *Encoder Model*. The encoder model outputs the processed context vector, which serves as input to the decoder.\n",
    "\n",
    "\n",
    "**Decoder: Generating the Output Sequence**\n",
    "\n",
    "The decoder takes the context vector from the encoder and generates the output sequence one token at a time. This component is slightly more complex due to its recurrent nature.\n",
    "\n",
    "1. *Decoder's Initial State*. The decoder begins with the context vector (output of the encoder) as its initial state, which serves as the memory of the source sequence.\n",
    "2. *Input Layer*. The decoder has a separate input layer ```decoder_input``` for target text sequences.\n",
    "3. *Text Vectorization and Embedding*. Similar to the encoder, the target input is passed through a ```vectorizer``` and then through an ```Embedding``` layer. The embedding layer for the decoder is separate since the source and target vocabularies differ.\n",
    "4. *GRU Layer*. The GRU layer processes the embedded vectors to predict the next word. Unlike the encoder, the GRU here is unidirectional because predictions are sequential, depending only on past and current inputs.\n",
    "   - The initial state of the GRU is set to the context vector (decoder_initial_state) from the encoder.\n",
    "   - return_sequences=True ensures that the GRU outputs a sequence of states, one for each time step.\n",
    "5. *Dense Layers*. Two fully connected layers:\n",
    "6. *Decoder Model*. The decoder outputs probabilities for the next word at each time step, enabling sequence generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad646ec-d269-4511-875f-e78c99f1e11a",
   "metadata": {},
   "source": [
    "![seq-seq-modeling](plots/en2ger-translator.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca240dc5-69e5-457d-a0ce-e2797f9b353d",
   "metadata": {},
   "source": [
    "### **Step 9**: Prepare and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b2cda29-617e-4b62-afb5-b28fb6c1840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    Create a data dictionary from the dataframes containing data\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for label, df in zip(\n",
    "        ['train', 'valid', 'test'], [train_df, valid_df, test_df]\n",
    "        ):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(\n",
    "        df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist()\n",
    "        )\n",
    "        de_labels = np.array(\n",
    "        df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist()\n",
    "        )\n",
    "        data_dict[label] = {\n",
    "        'encoder_inputs': en_inputs,\n",
    "        'decoder_inputs': de_inputs,\n",
    "        'decoder_labels': de_labels\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "877a2ba7-7df7-4e6f-82d0-a5e9c5a9a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_indices=None):\n",
    "    \"\"\"\n",
    "    Shuffle the data randomly (but all of inputs and labels at ones)\n",
    "    \"\"\"\n",
    "    if shuffle_indices is None:\n",
    "        shuffle_indices = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "    return (\n",
    "    en_inputs[shuffle_indices],\n",
    "    de_inputs[shuffle_indices],\n",
    "    de_labels[shuffle_indices]\n",
    "    ), shuffle_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfa886-a9bf-41b0-b161-fd6fbc1646ab",
   "metadata": {},
   "source": [
    "### **Step 10**: Define BLEU Evaluation metric\n",
    "---\n",
    "\n",
    "The source of the ```nmt_bleu.py``` module can be found in this [GIT Repository](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0695f8e5-3ec9-4b6c-a726-3fb8b8ad3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from nmt_bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric:\n",
    "    def __init__(self, vocabulary, name='bleu', **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the BLEU score for machine translation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(\n",
    "            vocabulary=self.vocab,\n",
    "            invert=True,\n",
    "            num_oov_indices=1  # Allow one OOV token\n",
    "        )\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score for targets and predictions.\n",
    "        \"\"\"\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  # Get predicted IDs\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)  # Convert to tokens\n",
    "        real_tokens = self.id_to_token_layer(real)  # Convert to tokens\n",
    "        \n",
    "        # Clean and process tokens\n",
    "        def clean_text(tokens):\n",
    "            # Remove padding, strip unwanted tokens\n",
    "            t = tf.strings.strip(\n",
    "                tf.strings.regex_replace(\n",
    "                    tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                    \"eos.*\", \"\"  # Remove everything after \"eos\"\n",
    "                )\n",
    "            )\n",
    "            t = np.char.decode(t.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "            t = [doc if len(doc) > 0 else '[UNK]' for doc in t]\n",
    "            return np.char.split(t).tolist()\n",
    "        \n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        real_tokens = [[r] for r in clean_text(real_tokens)]  # Format for BLEU\n",
    "\n",
    "        # Compute BLEU using the provided `compute_bleu` function\n",
    "        bleu, _, _, _, _, _ = compute_bleu(real_tokens, pred_tokens)\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4091ed-16e8-4087-ae34-6dd2838f08d6",
   "metadata": {},
   "source": [
    "**Explanation**: The *BLEU* metric measures the similarity between predicted translations and ground truth references. It is a widely used evaluation metric for machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b96c0b-7eb4-4599-aee6-883bb53c8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tokens):\n",
    "    \n",
    "\n",
    "    # 3. Strip the string of any extra white spaces\n",
    "    translations_in_bytes = tf.strings.strip(\n",
    "        # 2. Replace everything after the eos token with blank\n",
    "        tf.strings.regex_replace(\n",
    "            # 1. Join all the tokens to one string in each sequence\n",
    "            tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                \"eos.*\", \"\"\n",
    "                ),\n",
    "                )\n",
    "    # Decode the byte stream to a string\n",
    "    translations = np.char.decode(translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "    \n",
    "    # If the string is empty, add a [UNK] token\n",
    "    # Otherwise get a Division by zero error\n",
    "    translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
    "    \n",
    "    # Split the sequences to individual tokens\n",
    "    translations = np.char.split(translations).tolist()\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "294a0aed-f67f-47e8-9260-49731a63ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, epochs, batch_size):\n",
    "    \"\"\"Evaluate the model on various metrics.\"\"\"\n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        print(f\"Evaluating batch {i + 1}/{n_batches}\", end=\"\\r\")\n",
    "\n",
    "        # Convert inputs to tensors\n",
    "        x = [\n",
    "            tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "        ]\n",
    "        # Convert labels to integer token IDs\n",
    "        y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "        # Evaluate model\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu)\n",
    "\n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff5c1bb-5cfa-4d6e-ba97-d34819950c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model to save.\n",
    "        model_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    model.save(model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "    \n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\"Train the model and evaluate on validation/test sets.\"\"\"\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        bleu_log, accuracy_log, loss_log = [], [], []\n",
    "\n",
    "        # Shuffle the training data\n",
    "        (en_inputs_raw, de_inputs_raw, de_labels_raw), shuffle_inds = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training Loop\n",
    "        for i in range(n_train_batches):\n",
    "            print(f\"Training batch {i + 1}/{n_train_batches}\", end=\"\\r\")\n",
    "            x = [\n",
    "                tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "                tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            ]\n",
    "            y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "            # Train on batch\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "            # Track metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            pred_y = model.predict(x)\n",
    "            bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu)\n",
    "\n",
    "        print(f\"\\t(train) loss: {np.mean(loss_log):.4f} - accuracy: {np.mean(accuracy_log):.4f} - bleu: {np.mean(bleu_log):.4f}\")\n",
    "\n",
    "        # Validation after the epoch\n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model,\n",
    "            vectorizer,\n",
    "            data_dict['valid']['encoder_inputs'],\n",
    "            data_dict['valid']['decoder_inputs'],\n",
    "            data_dict['valid']['decoder_labels'],\n",
    "            epochs=1,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        print(f\"\\t(valid) loss: {val_loss:.4f} - accuracy: {val_accuracy:.4f} - bleu: {val_bleu:.4f}\")\n",
    "\n",
    "    # Test evaluation after all epochs\n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        data_dict['test']['encoder_inputs'],\n",
    "        data_dict['test']['decoder_inputs'],\n",
    "        data_dict['test']['decoder_labels'],\n",
    "        epochs=1,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(f\"\\n(test) loss: {test_loss:.4f} - accuracy: {test_accuracy:.4f} - bleu: {test_bleu:.4f}\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    model_path = \"en2de_translatorX.h5\"\n",
    "    save_model(seq2seq_model, model_path)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e0f6322-2e92-4d23-b051-1049bb4a6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Training batch 1/38\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sulai\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['encoder_input_final', 'decoder_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000226CE566830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000226CE566830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\t(train) loss: 5.9378 - accuracy: 0.0543 - bleu: 0.0000\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\t(valid) loss: 5.0835 - accuracy: 0.0720 - bleu: 0.0000\n",
      "\n",
      "Epoch 2/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\t(train) loss: 4.9618 - accuracy: 0.0777 - bleu: 0.0000\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\t(valid) loss: 4.7944 - accuracy: 0.0849 - bleu: 0.0000\n",
      "\n",
      "Epoch 3/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\t(train) loss: 4.6174 - accuracy: 0.0940 - bleu: 0.0000\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\t(valid) loss: 4.4166 - accuracy: 0.1052 - bleu: 0.0000\n",
      "\n",
      "Epoch 4/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\t(train) loss: 4.2069 - accuracy: 0.1117 - bleu: 0.0000\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\t(valid) loss: 4.0376 - accuracy: 0.1185 - bleu: 0.0000\n",
      "\n",
      "Epoch 5/5\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\t(train) loss: 3.8607 - accuracy: 0.1223 - bleu: 0.0074\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\t(valid) loss: 3.7739 - accuracy: 0.1273 - bleu: 0.0103\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(test) loss: 3.7593 - accuracy: 0.1277 - bleu: 0.0128\n",
      "Training complete.\n",
      "Model saved to en2de_translatorX.h5\n"
     ]
    }
   ],
   "source": [
    "train_model(seq2seq_model, de_vectorizer, train_df, valid_df, test_df,epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1591da2-1245-4f26-a3af-141636c4fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Loads the model from the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path from where the model will be loaded.\n",
    "        \n",
    "    Returns:\n",
    "        Loaded Keras model.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path, custom_objects={'TextVectorization': tf.keras.layers.TextVectorization})\n",
    "    print(f'Model loaded from {model_path}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d92a76-b9e3-4fac-a105-6ed68d1969c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef make_inference(model, en_vectorizer, input_sentence, de_vocab, max_output_length=20):\\n    \"\"\"\\n    Make inference using the trained seq2seq model.\\n    \\n    Args:\\n        model: Loaded seq2seq model.\\n        en_vectorizer: TextVectorization layer for the English vocabulary.\\n        input_sentence (str): Input English sentence to translate.\\n        de_vocab (list): List of German vocabulary words.\\n        max_output_length (int): Maximum length of the translated output sequence.\\n        \\n    Returns:\\n        str: Translated German sentence.\\n    \"\"\"\\n    # Vectorize the input sentence\\n    input_vector = tf.convert_to_tensor([input_sentence], dtype=tf.string)\\n    input_token_ids = en_vectorizer(input_vector)\\n\\n    # Initialize the context vector using the encoder\\n    context_vector = model.get_layer(\"Encoder\")(input_token_ids)\\n\\n    # Initialize the decoder input with the start token (e.g., \"sos\")\\n    start_token_index = de_vocab.index(\"sos\")\\n    decoder_input = tf.convert_to_tensor([[start_token_index]], dtype=tf.int32)\\n\\n    output_tokens = []\\n    for _ in range(max_output_length):\\n        # Make a prediction using the decoder\\n        logits = model.get_layer(\"Decoder\")([decoder_input, context_vector])\\n        predicted_token_id = tf.argmax(logits, axis=-1).numpy()[0, -1]\\n\\n        # If the end token is predicted, break the loop\\n        if de_vocab[predicted_token_id] == \"eos\":\\n            break\\n\\n        # Append the predicted token to the output tokens list\\n        output_tokens.append(de_vocab[predicted_token_id])\\n\\n        # Update the decoder input for the next step\\n        decoder_input = tf.convert_to_tensor([[predicted_token_id]], dtype=tf.int32)\\n\\n    # Join the output tokens to form the translated sentence\\n    translated_sentence = \" \".join(output_tokens)\\n    return translated_sentence\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def make_inference(model, en_vectorizer, input_sentence, de_vocab, max_output_length=20):\n",
    "    \"\"\"\n",
    "    Make inference using the trained seq2seq model.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded seq2seq model.\n",
    "        en_vectorizer: TextVectorization layer for the English vocabulary.\n",
    "        input_sentence (str): Input English sentence to translate.\n",
    "        de_vocab (list): List of German vocabulary words.\n",
    "        max_output_length (int): Maximum length of the translated output sequence.\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated German sentence.\n",
    "    \"\"\"\n",
    "    # Vectorize the input sentence\n",
    "    input_vector = tf.convert_to_tensor([input_sentence], dtype=tf.string)\n",
    "    input_token_ids = en_vectorizer(input_vector)\n",
    "\n",
    "    # Initialize the context vector using the encoder\n",
    "    context_vector = model.get_layer(\"Encoder\")(input_token_ids)\n",
    "\n",
    "    # Initialize the decoder input with the start token (e.g., \"sos\")\n",
    "    start_token_index = de_vocab.index(\"sos\")\n",
    "    decoder_input = tf.convert_to_tensor([[start_token_index]], dtype=tf.int32)\n",
    "\n",
    "    output_tokens = []\n",
    "    for _ in range(max_output_length):\n",
    "        # Make a prediction using the decoder\n",
    "        logits = model.get_layer(\"Decoder\")([decoder_input, context_vector])\n",
    "        predicted_token_id = tf.argmax(logits, axis=-1).numpy()[0, -1]\n",
    "\n",
    "        # If the end token is predicted, break the loop\n",
    "        if de_vocab[predicted_token_id] == \"eos\":\n",
    "            break\n",
    "\n",
    "        # Append the predicted token to the output tokens list\n",
    "        output_tokens.append(de_vocab[predicted_token_id])\n",
    "\n",
    "        # Update the decoder input for the next step\n",
    "        decoder_input = tf.convert_to_tensor([[predicted_token_id]], dtype=tf.int32)\n",
    "\n",
    "    # Join the output tokens to form the translated sentence\n",
    "    translated_sentence = \" \".join(output_tokens)\n",
    "    return translated_sentence\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6733d91-4c40-41d2-8300-46020c135e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = load_model_from_file(model_path='en2de_translatorX.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33a7e9ba-32e4-49d1-a9be-f0e2b6761e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sentence = \"How are you?\"\n",
    "# translated_sentence = make_inference(loaded_model, en_vectorizer, input_sentence, de_vocabulary)\n",
    "# print(f'Translated Sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1f142-8a7d-46c5-bed7-a3de6bfa810b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
