{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b803581-d8fb-47f1-9647-15a66f2fad93",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence Learning: English-German Translator\n",
    "\n",
    "This notebook illustrates how to develop an English-to-German translation model as a sequence-to-sequence (seq2seq) learning problem using TensorFlow and Keras. This project demonstrates the core components of an encoder-decoder architecture with *BiLSTMs*, handling of text preprocessing, and evaluation using BLEU scores.\n",
    "\n",
    "---\n",
    "\n",
    "### Sequence-to-Sequence Learning Overview\n",
    "\n",
    "Sequence-to-sequence (seq2seq) learning is a type of deep learning model that maps input sequences to output sequences of potentially different lengths. It has wide applications in:\n",
    "\n",
    "- Machine Translation (e.g., English-to-German)\n",
    "- Text Summarization\n",
    "- Chatbots and Conversational Agents\n",
    "\n",
    "Key components:\n",
    "- **Encoder**: Encodes the input sequence into a fixed-length context vector.\n",
    "- **Decoder**: Decodes the context vector to generate the target sequence.\n",
    "\n",
    "\n",
    "***Reference***: [TensorFlow in Action](https://www.google.de/books/edition/TensorFlow_in_Action/JYyKEAAAQBAJ?hl=en&gbpv=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab96504-1e7d-4489-8c35-a040e1054016",
   "metadata": {},
   "source": [
    "### **Step 1**: Download and Extract the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff7a8b5-c2a0-4e27-815b-1ea25619becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Ensure the required dataset is available and extracted\n",
    "def prepare_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the English-German dataset for translation.\n",
    "    Downloads and extracts the data if not already present.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing English and German sentences.\n",
    "    \"\"\"\n",
    "    data_dir = 'data'\n",
    "    zip_path = os.path.join(data_dir, 'deu-eng.zip')\n",
    "    extracted_path = os.path.join(data_dir, 'deu.txt')\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Please download 'deu-eng.zip' from \"\n",
    "            f\"http://www.manythings.org/anki/deu-eng.zip and place it in the {data_dir} folder.\"\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(extracted_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        print(\"Dataset extracted.\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "\n",
    "    # Load and preprocess the data\n",
    "    df = pd.read_csv(extracted_path, delimiter='\\t', header=None, names=[\"EN\", \"DE\", \"Attribution\"])\n",
    "    df = df[[\"EN\", \"DE\"]]\n",
    "    return df\n",
    "\n",
    "df = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55574daa-31e4-4f56-a0bc-ddb2c91073e8",
   "metadata": {},
   "source": [
    "**Explanation**: This step ensures the required dataset is extracted for processing. If the dataset is not present in the expected location, it prompts the user to download it manually.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325d0df-c72f-4574-a0ab-1b246f0af276",
   "metadata": {},
   "source": [
    "### **Step 2**: Data Setup and Preprocessing\n",
    "---\n",
    "\n",
    "Download [deu-eng dataset](http://www.manythings.org/anki/deu-eng.zip) manually and locate it in the ```\\data``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb765ef7-fb5f-43b4-9ec5-e70383c9a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (277891, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "\n",
    "print('df.shape = {}'.format(df.shape))\n",
    "\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "df = df.iloc[clean_inds]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01103c-7de1-411c-911e-2eb575762762",
   "metadata": {},
   "source": [
    "**Explanation**: In this step, we clean the data by removing rows with unwanted characters and retain only English and German sentence pairs for the translation task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0862044-6ebf-459b-86eb-072df1b974f5",
   "metadata": {},
   "source": [
    "### **Step 3**: Random Sampling and Token Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb3a317-1fe6-4c0d-b263-c076d10ab47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "random_seed= 4321\n",
    "df = df.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token\n",
    "\n",
    "# Randomly sample 10% examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Randomly sample 10% examples from the remaining randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f41bf-32cd-4de5-b52a-96a666166eee",
   "metadata": {},
   "source": [
    "**Explanation**: This step prepares a subset of ```50,000``` sentences for the task, appends start (```sos```) and end (```eos```) tokens to each German sentence, and splits the data into training, validation, and test sets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df912329-842f-429c-90d0-ee5842b112da",
   "metadata": {},
   "source": [
    "### **Step 4**: Analyze Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b17e00-6fb3-4947-b926-400af1763877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.294025\n",
      "std          2.542850\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39584.000000\n",
      "mean         6.184671\n",
      "std          2.284073\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.332250\n",
      "std          2.536094\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         52.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39227.000000\n",
      "mean         8.253116\n",
      "std          2.231582\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \"\"\"\n",
    "    Print the summary stats of the sequence length\n",
    "    \"\"\"\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    \n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    print(seq_length_ser[\n",
    "    (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n",
    "    ].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0efa8-3532-40f6-afef-135d3019df28",
   "metadata": {},
   "source": [
    "**Explanation**: Here, we analyze the sequence lengths for both corpora. The results help determine appropriate sequence lengths for the encoder and decoder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8a47d-6660-46b0-ab63-484f1c966d50",
   "metadata": {},
   "source": [
    "### **Step 5**: Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef74d93d-ae69-4e43-ac28-d4e59f2d8024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9228\n",
      "to     8700\n",
      "I      8620\n",
      "the    6766\n",
      "you    6136\n",
      "a      5741\n",
      "is     4141\n",
      "in     2639\n",
      "of     2470\n",
      "was    2380\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2218\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom       9713\n",
      "Ich       7964\n",
      "ist       4735\n",
      "nicht     4616\n",
      "zu        3606\n",
      "Sie       3441\n",
      "du        3132\n",
      "das       2987\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2483\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \"\"\"\n",
    "    Get the vocabulary size above a certain threshold\n",
    "    \"\"\"\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    freq_df = pd.Series(\n",
    "    list(counter.values()),\n",
    "    index=list(counter.keys())\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(freq_df.head(n=10))\n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05abad-518a-4a73-beaa-66fd33cfeeb9",
   "metadata": {},
   "source": [
    "**Explanation**: We compute the vocabulary size for both languages, focusing on words that appear at least 10 times. This informs the TextVectorization step later in the pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46746d73-a993-4ddb-b133-02066e037147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2218\n",
      "DE vocabulary size: 2483\n",
      "\n",
      "\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "print(\"\\n\")\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae46e890-6eff-4310-b2b6-3ede8425e2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.294025\n",
      "std          2.542850\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39584.000000\n",
      "mean         6.184671\n",
      "std          2.284073\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.332250\n",
      "std          2.536094\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         52.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39227.000000\n",
      "mean         8.253116\n",
      "std          2.231582\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_sequence_length(str_ser):\n",
    "    \"\"\"\n",
    "    Print the summary stats of the sequence length\n",
    "    \"\"\"\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    \n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    print(seq_length_ser[\n",
    "    (seq_length_ser >= p_01) & (seq_length_ser < p_99)\n",
    "    ].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a33d30-ed5b-4408-a7f1-1c2db38851e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2218\n",
      "DE vocabulary size: 2483\n",
      "\n",
      "\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "print(\"\\n\")\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08484d2-f80f-496e-977c-fade1e87c535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import TextVectorization\\n\\nen_vectorize_layer = TextVectorization(\\nmax_tokens=en_vocab,\\noutput_mode=\\'int\\',\\noutput_sequence_length=None\\n)\\n\\nen_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype(\\'str\\'))\\nprint(en_vectorize_layer.get_vocabulary()[:10])\\nprint(len(en_vectorize_layer.get_vocabulary()))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "en_vectorize_layer = TextVectorization(\n",
    "max_tokens=en_vocab,\n",
    "output_mode='int',\n",
    "output_sequence_length=None\n",
    ")\n",
    "\n",
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "print(len(en_vectorize_layer.get_vocabulary()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9537d61e-f552-47b9-9150-9a2a3f068523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow.keras import Sequential\\nfrom tensorflow.keras.layers import TextVectorization\\n\\n# Vocabulary size and sequence length\\nen_vocab = 1000\\n\\n# Define the TextVectorization layer\\nen_vectorize_layer = TextVectorization(\\n    max_tokens=en_vocab,\\n    output_mode=\\'int\\',\\n    output_sequence_length=10  # Adjust sequence length as needed\\n)\\n\\n# Adapt the TextVectorization layer to a sample dataset\\nsample_text = [\"run\", \"how are you\", \"ectoplasmic residue\"]\\nen_vectorize_layer.adapt(sample_text)\\n\\n# Create a toy model with the TextVectorization layer\\ntoy_model = Sequential([en_vectorize_layer])\\n\\n# Input data (ensure it\\'s a TensorFlow tensor of strings)\\ninput_data = tf.constant([\"run\", \"how are you\", \"ectoplasmic residue\"])\\n\\n# Predict token IDs using the model\\npred = toy_model.predict(input_data)\\n\\n# Display results\\nprint(\"Input data: \\n{}\\n\".format(input_data.numpy()))\\nprint(\"\\nToken IDs: \\n{}\".format(pred))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Vocabulary size and sequence length\n",
    "en_vocab = 1000\n",
    "\n",
    "# Define the TextVectorization layer\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=10  # Adjust sequence length as needed\n",
    ")\n",
    "\n",
    "# Adapt the TextVectorization layer to a sample dataset\n",
    "sample_text = [\"run\", \"how are you\", \"ectoplasmic residue\"]\n",
    "en_vectorize_layer.adapt(sample_text)\n",
    "\n",
    "# Create a toy model with the TextVectorization layer\n",
    "toy_model = Sequential([en_vectorize_layer])\n",
    "\n",
    "# Input data (ensure it's a TensorFlow tensor of strings)\n",
    "input_data = tf.constant([\"run\", \"how are you\", \"ectoplasmic residue\"])\n",
    "\n",
    "# Predict token IDs using the model\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "# Display results\n",
    "print(\"Input data: \\n{}\\n\".format(input_data.numpy()))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ddd86f4-b30a-4f1a-9822-00ca49b9d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(\n",
    "    corpus, n_vocab=100, max_length=None, return_vocabulary=True, name=None\n",
    "    ):\n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "                                                        max_tokens=n_vocab+2,\n",
    "                                                        output_mode='int',\n",
    "                                                        output_sequence_length=max_length,\n",
    "                                                        )\n",
    "    vectorize_layer.adapt(corpus)\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    if not return_vocabulary:\n",
    "        return tf.keras.models.Model(\n",
    "    inputs=inp, outputs=vectorized_out, name=name\n",
    "    )\n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f54366e4-512b-4637-96fd-78fd3700d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\n",
    "max_length=en_seq_length, name='en_vectorizer'\n",
    ")\n",
    "\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n",
    "max_length=de_seq_length-1, name='de_vectorizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68de6e87-380d-4a50-b0fc-3fccde705aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn_vocab = 100\\n\\n# The input is (None,1) shaped and accepts an array of strings\\ninp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\\n\\n# Vectorize the data (assign token IDs)\\nvectorized_out = en_vectorizer(inp)\\n\\n# Define an embedding layer to convert IDs to word vectors\\nemb_layer = tf.keras.layers.Embedding(\\ninput_dim=n_vocab+2, output_dim=128, mask_zero=True, name='e_embedding')\\n\\n# Get the embeddings of the token IDs\\nemb_out = emb_layer(vectorized_out)\\n\\ngru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128))\\ngru_out = gru_layer(emb_out)\\n\\nencoder = tf.keras.models.Model(inputs=inp, outputs=gru_out)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "n_vocab = 100\n",
    "\n",
    "# The input is (None,1) shaped and accepts an array of strings\n",
    "inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "# Vectorize the data (assign token IDs)\n",
    "vectorized_out = en_vectorizer(inp)\n",
    "\n",
    "# Define an embedding layer to convert IDs to word vectors\n",
    "emb_layer = tf.keras.layers.Embedding(\n",
    "input_dim=n_vocab+2, output_dim=128, mask_zero=True, name='e_embedding')\n",
    "\n",
    "# Get the embeddings of the token IDs\n",
    "emb_out = emb_layer(vectorized_out)\n",
    "\n",
    "gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128))\n",
    "gru_out = gru_layer(emb_out)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366a3dac-5185-4dd8-9720-2fb91ca4a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_encoder(n_vocab, vectorizer):\n",
    "    \"\"\"\n",
    "    Build the encoder for the seq2seq model.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # Text vectorization\n",
    "    vectorized_output = vectorizer(encoder_input)\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=n_vocab + 2, \n",
    "        output_dim=128, \n",
    "        mask_zero=True, \n",
    "        name='encoder_embedding'\n",
    "    )\n",
    "    embedded_output = embedding_layer(vectorized_output)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    bidirectional_gru = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, name='encoder_gru'), \n",
    "        name='encoder_bidirectional_gru'\n",
    "    )\n",
    "    encoder_output = bidirectional_gru(embedded_output)\n",
    "    \n",
    "    # Define encoder model\n",
    "    encoder_model = tf.keras.models.Model(\n",
    "        inputs=encoder_input, \n",
    "        outputs=encoder_output, \n",
    "        name='encoder'\n",
    "    )\n",
    "    return encoder_model\n",
    "\n",
    "\n",
    "def build_decoder(n_vocab, encoder, vectorizer):\n",
    "    \"\"\"\n",
    "    Build the decoder for the seq2seq model.\n",
    "    \"\"\"\n",
    "    # Encoder input for initializing the decoder's GRU state\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input_final')\n",
    "    decoder_initial_state = encoder(encoder_input)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='decoder_input')\n",
    "    \n",
    "    # Text vectorization for decoder input\n",
    "    vectorized_output = vectorizer(decoder_input)\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=n_vocab + 2, \n",
    "        output_dim=128, \n",
    "        mask_zero=True, \n",
    "        name='decoder_embedding'\n",
    "    )\n",
    "    embedded_output = embedding_layer(vectorized_output)\n",
    "    \n",
    "    # GRU layer\n",
    "    gru_layer = tf.keras.layers.GRU(\n",
    "        256, \n",
    "        return_sequences=True, \n",
    "        name='decoder_gru'\n",
    "    )\n",
    "    gru_output = gru_layer(embedded_output, initial_state=decoder_initial_state)\n",
    "    \n",
    "    # Dense layers for output\n",
    "    dense_layer_1 = tf.keras.layers.Dense(\n",
    "        512, \n",
    "        activation='relu', \n",
    "        name='decoder_dense_1'\n",
    "    )\n",
    "    dense_output_1 = dense_layer_1(gru_output)\n",
    "    \n",
    "    dense_layer_final = tf.keras.layers.Dense(\n",
    "        n_vocab + 2, \n",
    "        activation='softmax', \n",
    "        name='decoder_dense_final'\n",
    "    )\n",
    "    decoder_output = dense_layer_final(dense_output_1)\n",
    "    \n",
    "    # Define decoder model\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        inputs=[encoder_input, decoder_input], \n",
    "        outputs=decoder_output, \n",
    "        name='decoder'\n",
    "    )\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a453331-d11a-4a43-85da-29ec13f02417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ne_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\\nd_init_state = encoder(e_inp)\\n\\nd_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\\nvectorized_out = de_vectorizer(inp)\\n\\nemb_layer = tf.keras.layers.Embedding(\\ninput_dim=n_vocab+2, output_dim=128, mask_zero=True, name='d_embedding'\\n)\\nemb_out = emb_layer(vectorized_out)\\n\\ngru_layer = tf.keras.layers.GRU(256, return_sequences=True)\\ngru_out = gru_layer(emb_out, initial_state=d_init_state)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')\n",
    "d_init_state = encoder(e_inp)\n",
    "\n",
    "d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "vectorized_out = de_vectorizer(inp)\n",
    "\n",
    "emb_layer = tf.keras.layers.Embedding(\n",
    "input_dim=n_vocab+2, output_dim=128, mask_zero=True, name='d_embedding'\n",
    ")\n",
    "emb_out = emb_layer(vectorized_out)\n",
    "\n",
    "gru_layer = tf.keras.layers.GRU(256, return_sequences=True)\n",
    "gru_out = gru_layer(emb_out, initial_state=d_init_state)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5707b356-9a4b-411a-9bed-00ea6af3f0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ en_vectorizer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">284,160</span> │ en_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ en_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bidirectio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,144</span> │ encoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ en_vectorizer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m284,160\u001b[0m │ en_vectorizer[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ en_vectorizer[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_bidirectio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m198,144\u001b[0m │ encoder_embeddin… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">482,304</span> (1.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m482,304\u001b[0m (1.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">482,304</span> (1.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m482,304\u001b[0m (1.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ de_vectorizer       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_input_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">318,080</span> │ de_vectorizer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">482,304</span> │ encoder_input_fi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ decoder_embeddin… │\n",
       "│                     │                   │            │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ decoder_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2485</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,274,805</span> │ decoder_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ de_vectorizer       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_input_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m318,080\u001b[0m │ de_vectorizer[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m482,304\u001b[0m │ encoder_input_fi… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_gru (\u001b[38;5;33mGRU\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ decoder_embeddin… │\n",
       "│                     │                   │            │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m131,584\u001b[0m │ decoder_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dense_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2485\u001b[0m)  │  \u001b[38;5;34m1,274,805\u001b[0m │ decoder_dense_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"seq2seq_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2485</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> │ encoder_input_fi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2485\u001b[0m)  │  \u001b[38;5;34m2,503,221\u001b[0m │ encoder_input_fi… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_seq2seq_model(encoder, decoder):\n",
    "    \"\"\"\n",
    "    Build the final seq2seq model using the pre-built encoder and decoder.\n",
    "    \"\"\"\n",
    "    # Encoder input\n",
    "    encoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input_final')\n",
    "    encoder_output = encoder(encoder_input)\n",
    "\n",
    "    # Decoder input\n",
    "    decoder_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='decoder_input')\n",
    "\n",
    "    # Use the decoder model\n",
    "    decoder_output = decoder([encoder_input, decoder_input])\n",
    "\n",
    "    # Define the seq2seq model\n",
    "    seq2seq_model = tf.keras.models.Model(\n",
    "        inputs=[encoder_input, decoder_input], \n",
    "        outputs=decoder_output, \n",
    "        name='seq2seq_model'\n",
    "    )\n",
    "    return seq2seq_model\n",
    "\n",
    "\n",
    "# Usage\n",
    "# Assuming `en_vocab`, `en_vectorizer`, and `de_vectorizer` are defined\n",
    "encoder = build_encoder(en_vocab, en_vectorizer)\n",
    "decoder = build_decoder(de_vocab, encoder, de_vectorizer)\n",
    "seq2seq_model = build_seq2seq_model(encoder, decoder)\n",
    "\n",
    "# Summaries\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b4a3f52-4347-4968-99fe-b2f3301638b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Get the English vectorizer/vocabulary\\nen_vectorizer, en_vocabulary = get_vectorizer(\\ncorpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\\nmax_length=en_seq_length, name=\\'e_vectorizer\\'\\n)\\n# Get the German vectorizer/vocabulary\\nde_vectorizer, de_vocabulary = get_vectorizer(corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\\nmax_length=de_seq_length-1, name=\\'d_vectorizer\\'\\n)\\n# Define the final model\\nencoder = get_encoder(n_vocab=en_vocab, vectorizer=en_vectorizer)\\nfinal_model = get_final_seq2seq_model(\\nn_vocab=de_vocab, encoder=encoder, vectorizer=de_vectorizer\\n)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(\n",
    "corpus=np.array(train_df[\"EN\"].tolist()), n_vocab=en_vocab,\n",
    "max_length=en_seq_length, name='e_vectorizer'\n",
    ")\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(corpus=np.array(train_df[\"DE\"].tolist()), n_vocab=de_vocab,\n",
    "max_length=de_seq_length-1, name='d_vectorizer'\n",
    ")\n",
    "# Define the final model\n",
    "encoder = get_encoder(n_vocab=en_vocab, vectorizer=en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(\n",
    "n_vocab=de_vocab, encoder=encoder, vectorizer=de_vectorizer\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3060d46e-39a3-431a-8f28-e864689edba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure [UNK] is at the beginning of the vocabulary\n",
    "en_vocabulary = [v for v in en_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "en_vocabulary = ['[UNK]'] + en_vocabulary  # Add [UNK] at the start\n",
    "\n",
    "de_vocabulary = [v for v in de_vocabulary if v != '[UNK]']  # Remove existing [UNK]\n",
    "de_vocabulary = ['[UNK]'] + de_vocabulary  # Add [UNK] at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5edab6bc-638f-458c-9ccd-408ca7f09547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "seq2seq_model.compile(\n",
    "loss='sparse_categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac0f92e-b83c-416c-930f-3591b3c97cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"seq2seq_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"seq2seq_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2485</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> │ encoder_input_fi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input_final │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2485\u001b[0m)  │  \u001b[38;5;34m2,503,221\u001b[0m │ encoder_input_fi… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,503,221</span> (9.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,503,221\u001b[0m (9.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b2cda29-617e-4b62-afb5-b28fb6c1840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    Create a data dictionary from the dataframes containing data\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for label, df in zip(\n",
    "        ['train', 'valid', 'test'], [train_df, valid_df, test_df]\n",
    "        ):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(\n",
    "        df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist()\n",
    "        )\n",
    "        de_labels = np.array(\n",
    "        df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist()\n",
    "        )\n",
    "        data_dict[label] = {\n",
    "        'encoder_inputs': en_inputs,\n",
    "        'decoder_inputs': de_inputs,\n",
    "        'decoder_labels': de_labels\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "877a2ba7-7df7-4e6f-82d0-a5e9c5a9a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_indices=None):\n",
    "    \"\"\"\n",
    "    Shuffle the data randomly (but all of inputs and labels at ones)\n",
    "    \"\"\"\n",
    "    if shuffle_indices is None:\n",
    "        shuffle_indices = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "    return (\n",
    "    en_inputs[shuffle_indices],\n",
    "    de_inputs[shuffle_indices],\n",
    "    de_labels[shuffle_indices]\n",
    "    ), shuffle_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0695f8e5-3ec9-4b6c-a726-3fb8b8ad3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from nmt_bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric:\n",
    "    def __init__(self, vocabulary, name='bleu', **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the BLEU score for machine translation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(\n",
    "            vocabulary=self.vocab,\n",
    "            invert=True,\n",
    "            num_oov_indices=1  # Allow one OOV token\n",
    "        )\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score for targets and predictions.\n",
    "        \"\"\"\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  # Get predicted IDs\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)  # Convert to tokens\n",
    "        real_tokens = self.id_to_token_layer(real)  # Convert to tokens\n",
    "        \n",
    "        # Clean and process tokens\n",
    "        def clean_text(tokens):\n",
    "            # Remove padding, strip unwanted tokens\n",
    "            t = tf.strings.strip(\n",
    "                tf.strings.regex_replace(\n",
    "                    tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                    \"eos.*\", \"\"  # Remove everything after \"eos\"\n",
    "                )\n",
    "            )\n",
    "            t = np.char.decode(t.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "            t = [doc if len(doc) > 0 else '[UNK]' for doc in t]\n",
    "            return np.char.split(t).tolist()\n",
    "        \n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        real_tokens = [[r] for r in clean_text(real_tokens)]  # Format for BLEU\n",
    "\n",
    "        # Compute BLEU using the provided `compute_bleu` function\n",
    "        bleu, _, _, _, _, _ = compute_bleu(real_tokens, pred_tokens)\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79a810ad-1e04-48b8-a6c3-59797f9da082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with longer correctly predicte phrases: 0.7598356856515925\n",
      "BLEU score without longer correctly predicte phrases: 0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "\n",
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "print(\"BLEU score with longer correctly predicte phrases: {}\".format(bleu1))\n",
    "print(\"BLEU score without longer correctly predicte phrases: {}\".format(bleu2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b96c0b-7eb4-4599-aee6-883bb53c8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tokens):\n",
    "    \n",
    "\n",
    "    # 3. Strip the string of any extra white spaces\n",
    "    translations_in_bytes = tf.strings.strip(\n",
    "        # 2. Replace everything after the eos token with blank\n",
    "        tf.strings.regex_replace(\n",
    "            # 1. Join all the tokens to one string in each sequence\n",
    "            tf.strings.join(tf.transpose(tokens), separator=' '),\n",
    "                \"eos.*\", \"\"\n",
    "                ),\n",
    "                )\n",
    "    # Decode the byte stream to a string\n",
    "    translations = np.char.decode(translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "    \n",
    "    # If the string is empty, add a [UNK] token\n",
    "    # Otherwise get a Division by zero error\n",
    "    translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
    "    \n",
    "    # Split the sequences to individual tokens\n",
    "    translations = np.char.split(translations).tolist()\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52dfee0e-f06f-4ec8-84ef-b9d0af647b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung','bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "075764a2-00fc-420a-998b-bd6eec33872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung','bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a414479-f287-4f52-83c2-d1ba81d5b157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with longer correctly predict phrases: 0.7598356856515925\n",
      "BLEU score without longer correctly predict phrases:0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "print(\"BLEU score with longer correctly predict phrases: {}\".format(bleu1))\n",
    "print(\"BLEU score without longer correctly predict phrases:{}\".format(bleu2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "294a0aed-f67f-47e8-9260-49731a63ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, epochs, batch_size):\n",
    "    \"\"\"Evaluate the model on various metrics.\"\"\"\n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        print(f\"Evaluating batch {i + 1}/{n_batches}\", end=\"\\r\")\n",
    "\n",
    "        # Convert inputs to tensors\n",
    "        x = [\n",
    "            tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "        ]\n",
    "        # Convert labels to integer token IDs\n",
    "        y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "        # Evaluate model\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Compute BLEU score\n",
    "        bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu)\n",
    "\n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ff5c1bb-5cfa-4d6e-ba97-d34819950c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model to save.\n",
    "        model_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    model.save(model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "    \n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\"Train the model and evaluate on validation/test sets.\"\"\"\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        bleu_log, accuracy_log, loss_log = [], [], []\n",
    "\n",
    "        # Shuffle the training data\n",
    "        (en_inputs_raw, de_inputs_raw, de_labels_raw), shuffle_inds = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training Loop\n",
    "        for i in range(n_train_batches):\n",
    "            print(f\"Training batch {i + 1}/{n_train_batches}\", end=\"\\r\")\n",
    "            x = [\n",
    "                tf.convert_to_tensor(en_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "                tf.convert_to_tensor(de_inputs_raw[i * batch_size:(i + 1) * batch_size], dtype=tf.string),\n",
    "            ]\n",
    "            y = tf.convert_to_tensor(vectorizer(de_labels_raw[i * batch_size:(i + 1) * batch_size]))\n",
    "\n",
    "            # Train on batch\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "            # Track metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            pred_y = model.predict(x)\n",
    "            bleu = bleu_metric.calculate_bleu_from_predictions(y, pred_y)\n",
    "\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu)\n",
    "\n",
    "        print(f\"\\t(train) loss: {np.mean(loss_log):.4f} - accuracy: {np.mean(accuracy_log):.4f} - bleu: {np.mean(bleu_log):.4f}\")\n",
    "\n",
    "        # Validation after the epoch\n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model,\n",
    "            vectorizer,\n",
    "            data_dict['valid']['encoder_inputs'],\n",
    "            data_dict['valid']['decoder_inputs'],\n",
    "            data_dict['valid']['decoder_labels'],\n",
    "            epochs=1,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        print(f\"\\t(valid) loss: {val_loss:.4f} - accuracy: {val_accuracy:.4f} - bleu: {val_bleu:.4f}\")\n",
    "\n",
    "    # Test evaluation after all epochs\n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "        model,\n",
    "        vectorizer,\n",
    "        data_dict['test']['encoder_inputs'],\n",
    "        data_dict['test']['decoder_inputs'],\n",
    "        data_dict['test']['decoder_labels'],\n",
    "        epochs=1,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(f\"\\n(test) loss: {test_loss:.4f} - accuracy: {test_accuracy:.4f} - bleu: {test_bleu:.4f}\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    model_path = \"en2de_translatorX.h5\"\n",
    "    save_model(final_model, model_path)\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e0f6322-2e92-4d23-b051-1049bb4a6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Training batch 1/38\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sulai\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['encoder_input_final', 'decoder_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000200E4EF6560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x00000200E4EF6560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Training batch 8/38\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq2seq_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mde_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size)\u001b[0m\n\u001b[0;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_on_batch(x, y)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Track metrics\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x)\n\u001b[0;32m     52\u001b[0m bleu \u001b[38;5;241m=\u001b[39m bleu_metric\u001b[38;5;241m.\u001b[39mcalculate_bleu_from_predictions(y, pred_y)\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:432\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    431\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 432\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\Documents\\GH-Projects\\seq2seq-learning\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(seq2seq_model, de_vectorizer, train_df, valid_df, test_df,epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1591da2-1245-4f26-a3af-141636c4fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(model_path='seq2seq_translatorX.h5'):\n",
    "    \"\"\"\n",
    "    Loads the model from the specified file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path from where the model will be loaded.\n",
    "        \n",
    "    Returns:\n",
    "        Loaded Keras model.\n",
    "    \"\"\"\n",
    "    model = load_model(model_path, custom_objects={'TextVectorization': tf.keras.layers.TextVectorization})\n",
    "    print(f'Model loaded from {model_path}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d92a76-b9e3-4fac-a105-6ed68d1969c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(model, en_vectorizer, input_sentence, de_vocab, max_output_length=20):\n",
    "    \"\"\"\n",
    "    Make inference using the trained seq2seq model.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded seq2seq model.\n",
    "        en_vectorizer: TextVectorization layer for the English vocabulary.\n",
    "        input_sentence (str): Input English sentence to translate.\n",
    "        de_vocab (list): List of German vocabulary words.\n",
    "        max_output_length (int): Maximum length of the translated output sequence.\n",
    "        \n",
    "    Returns:\n",
    "        str: Translated German sentence.\n",
    "    \"\"\"\n",
    "    # Vectorize the input sentence\n",
    "    input_vector = tf.convert_to_tensor([input_sentence], dtype=tf.string)\n",
    "    input_token_ids = en_vectorizer(input_vector)\n",
    "\n",
    "    # Initialize the context vector using the encoder\n",
    "    context_vector = model.get_layer(\"Encoder\")(input_token_ids)\n",
    "\n",
    "    # Initialize the decoder input with the start token (e.g., \"sos\")\n",
    "    start_token_index = de_vocab.index(\"sos\")\n",
    "    decoder_input = tf.convert_to_tensor([[start_token_index]], dtype=tf.int32)\n",
    "\n",
    "    output_tokens = []\n",
    "    for _ in range(max_output_length):\n",
    "        # Make a prediction using the decoder\n",
    "        logits = model.get_layer(\"Decoder\")([decoder_input, context_vector])\n",
    "        predicted_token_id = tf.argmax(logits, axis=-1).numpy()[0, -1]\n",
    "\n",
    "        # If the end token is predicted, break the loop\n",
    "        if de_vocab[predicted_token_id] == \"eos\":\n",
    "            break\n",
    "\n",
    "        # Append the predicted token to the output tokens list\n",
    "        output_tokens.append(de_vocab[predicted_token_id])\n",
    "\n",
    "        # Update the decoder input for the next step\n",
    "        decoder_input = tf.convert_to_tensor([[predicted_token_id]], dtype=tf.int32)\n",
    "\n",
    "    # Join the output tokens to form the translated sentence\n",
    "    translated_sentence = \" \".join(output_tokens)\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6733d91-4c40-41d2-8300-46020c135e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model_from_file(model_path='en2de_translatorX.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7e9ba-32e4-49d1-a9be-f0e2b6761e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"How are you?\"\n",
    "translated_sentence = make_inference(loaded_model, en_vectorizer, input_sentence, de_vocabulary)\n",
    "print(f'Translated Sentence: {translated_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1f142-8a7d-46c5-bed7-a3de6bfa810b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
